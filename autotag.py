# ==============================================
# KOICA TAG v4.0 - ì„¹í„° ì „ë¬¸ê°€ ì§‘ì¤‘ + Qwen2.5 32B
# ==============================================
#
# ğŸ”¥ v4.0 ì£¼ìš” ë³€ê²½:
# 1. PMC Agent ì œê±° â†’ LLM í˜¸ì¶œ 6íšŒ â†’ 1íšŒë¡œ ëŒ€í­ ì¶•ì†Œ
# 2. ì„¹í„° ì „ë¬¸ê°€ ë¶„ì„ë§Œ ì§‘ì¤‘ â†’ ì„¹í„°ë³„ í•µì‹¬ ì´ìŠˆ + í•„ìˆ˜ ì§ˆë¬¸ ë¹¡ì„¸ê²Œ ê²€í† 
# 3. ì²˜ë¦¬ ì†ë„ ëŒ€í­ í–¥ìƒ â†’ Agent ë¶€ë‹´ ê°ì†Œë¡œ ì•½ 5~6ë°° ë¹ ë¦„
# 4. ê²€í†  í’ˆì§ˆ ê°•í™” â†’ ì„¹í„° ì „ë¬¸ì„±ì— ì§‘ì¤‘í•œ ì‹¬ì¸µ ë¶„ì„
# 5. AI ì •ì‹  ì°¨ë¦¼ â†’ í•œ ë²ˆì— í•˜ë‚˜ì˜ ì—­í• ë§Œ ìˆ˜í–‰
# 6. Qwen2.5 32B â†’ ìµœì‹  ëª¨ë¸, ìš°ìˆ˜í•œ ì„±ëŠ¥, ë¹ ë¥¸ ì†ë„ (A100 40GB ìµœì )
# ==============================================

import torch
import gc
import time
from collections import defaultdict
from typing import List, Dict, Tuple, Optional
import re

assert torch.cuda.is_available(), "âŒ GPU ëŸ°íƒ€ì„ì´ ì•„ë‹™ë‹ˆë‹¤!"
print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
print(f"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ëŠ” Colab ë…¸íŠ¸ë¶ì—ì„œ ë³„ë„ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:
# !pip install -q pdfplumber gradio sentence-transformers huggingface-hub
# !pip install -q llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
# !pip install -q pandas numpy

print("\nâœ… GPU í™•ì¸ ì™„ë£Œ! íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ ì¤‘...\n")

from huggingface_hub import hf_hub_download
from llama_cpp import Llama
from sentence_transformers import SentenceTransformer
import gradio as gr
import pdfplumber
import numpy as np
import pandas as pd

print("ğŸ“¥ Qwen2.5 32B ë‹¤ìš´ë¡œë“œ ì¤‘...")

model_path = hf_hub_download(
    repo_id="Qwen/Qwen2.5-32B-Instruct-GGUF",
    filename="qwen2.5-32b-instruct-q4_k_m.gguf"
)

print("ğŸ”„ LLM ì´ˆê¸°í™” ì¤‘...")
llm = Llama(
    model_path=model_path,
    n_ctx=16384,       # Qwen2.5: 128K context ì§€ì› (16Kë¡œ ì„¤ì •)
    n_gpu_layers=-1,   # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì— ë¡œë“œ (32BëŠ” A100 40GBì— ì í•©)
    n_batch=512,
    n_threads=4,
    verbose=False
)
print("âœ… LLM ì¤€ë¹„ ì™„ë£Œ! (Qwen2.5 32B Instruct)\n")

print("ğŸ”„ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
try:
    embedder = SentenceTransformer('jhgan/ko-sroberta-multitask', device='cpu')
    print("âœ… í•œêµ­ì–´ ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ! (CPU)\n")
except:
    embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device='cpu')
    print("âœ… ë‹¤êµ­ì–´ ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ! (CPU)\n")

if 'demo' in dir():
    try:
        demo.close()
        del demo
        gc.collect()
    except:
        pass

timing_stats = defaultdict(list)

def track_time(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        timing_stats[func.__name__].append(elapsed)
        print(f"  â±ï¸ {func.__name__}: {elapsed:.2f}ì´ˆ")
        return result
    return wrapper


def generate_with_validation(
    messages: List[Dict],
    vector_db: Optional[Dict] = None,
    max_retries: int = 2,
    max_tokens: int = 6000
) -> str:
    """ê²€ì¦ + ì¬ìƒì„± ë£¨í”„: ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ì¬ìƒì„±"""

    for attempt in range(max_retries + 1):
        print(f"  ğŸ”„ ìƒì„± ì‹œë„ {attempt + 1}/{max_retries + 1}")

        response = llm.create_chat_completion(
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.3,
            top_p=0.95,
            top_k=50,
            repeat_penalty=1.1,
            stop=["[ì§ˆë¬¸]", "[êµ¬ì²´ì ]", "[í˜ì´ì§€]", "[ê¸ˆì•¡]", "[ì¡°ì§]", "[ë‹´ë‹¹]"]
        )

        output = response['choices'][0]['message']['content']
        output = comprehensive_post_processing(output, "ê²€ì¦ëŒ€ìƒ")

        # ê²€ì¦
        is_valid, issues = validate_analysis_logic(output, vector_db)

        if is_valid:
            print(f"  âœ… ê²€ì¦ í†µê³¼!")
            return output
        else:
            # ê²€ì¦ ì‹¤íŒ¨ - ê²½ê³  ì¶œë ¥
            warnings = "\n".join([f"    - {i['type']}: {i['desc']}" for i in issues])
            print(f"  âš ï¸ ê²€ì¦ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}):\n{warnings}")

            if attempt < max_retries:
                # ì¬ì‹œë„ - ì´ì „ ì˜¤ë¥˜ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
                error_feedback = "\n\nğŸš¨ **ì´ì „ ì‹œë„ì—ì„œ ë°œê²¬ëœ ì˜¤ë¥˜**:\n"
                for i, issue in enumerate(issues[:3], 1):  # ìµœëŒ€ 3ê°œë§Œ
                    error_feedback += f"{i}. {issue['type']}: {issue['desc']}\n"
                error_feedback += "\nìœ„ ì˜¤ë¥˜ë¥¼ **ë°˜ë“œì‹œ ìˆ˜ì •**í•˜ì—¬ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”."

                # ë§ˆì§€ë§‰ user ë©”ì‹œì§€ì— í”¼ë“œë°± ì¶”ê°€
                messages[-1]['content'] += error_feedback
            else:
                # ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬ - ê·¸ëƒ¥ ë°˜í™˜
                print(f"  âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬. ê²€ì¦ ì‹¤íŒ¨ ìƒíƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return output

    return output

# ==============================================
# RAG í•¨ìˆ˜ë“¤ (v2.9ì™€ ë™ì¼)
# ==============================================

def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 400) -> List[Dict]:
    chunks = []
    start = 0
    chunk_id = 0
    
    page_markers = [m.start() for m in re.finditer(r'(í˜ì´ì§€\s*\d+|Page\s*\d+|\f)', text)]
    
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk_text = text[start:end]
        
        estimated_page = 1
        for marker_pos in page_markers:
            if marker_pos <= start:
                estimated_page += 1
        
        if not page_markers:
            estimated_page = (start // 2000) + 1
        
        chunks.append({
            "id": chunk_id,
            "text": chunk_text,
            "start": start,
            "end": end,
            "page": estimated_page
        })
        
        chunk_id += 1
        start = end - overlap
        if end >= len(text):
            break
    
    return chunks


def create_vector_db(chunks: List[Dict], batch_size: int = 8) -> Dict:
    texts = [chunk["text"] for chunk in chunks]
    print(f"  ğŸ’¾ {len(chunks)}ê°œ ì²­í¬ ë²¡í„°í™” ì¤‘...")

    all_embeddings = []
    total_batches = (len(texts) + batch_size - 1) // batch_size

    for i in range(0, len(texts), batch_size):
        batch_num = i // batch_size + 1
        print(f"    â³ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...")
        batch = texts[i:i+batch_size]
        batch_emb = embedder.encode(
            batch,
            show_progress_bar=True,
            device='cpu',
            batch_size=batch_size
        )
        all_embeddings.append(batch_emb)
        print(f"    âœ… ë°°ì¹˜ {batch_num}/{total_batches} ì™„ë£Œ")

    embeddings = np.vstack(all_embeddings) if len(all_embeddings) > 1 else all_embeddings[0]
    print(f"  âœ… ë²¡í„°í™” ì™„ë£Œ!")

    return {"chunks": chunks, "embeddings": embeddings}


def _format_chunks(
    vector_db: Dict, 
    similarities: np.ndarray, 
    indices: np.ndarray, 
    fallback: bool = False
) -> Tuple[str, List[int]]:
    relevant_chunks = []
    page_numbers = []
    
    for i in indices:
        chunk = vector_db['chunks'][i]
        similarity = similarities[i]
        page_numbers.append(chunk['page'])
        
        if similarity > 0.6:
            context_len = 1500
            marker = "ğŸŸ¢"
        elif similarity > 0.4:
            context_len = 1200
            marker = "ğŸŸ¡"
        else:
            context_len = 900
            marker = "ğŸŸ " if not fallback else "âš ï¸"
        
        relevant_chunks.append(
            f"{marker} [p.{chunk['page']} | ê´€ë ¨ë„: {similarity:.1%}]\n{chunk['text'][:context_len]}"
        )
    
    if fallback:
        header = "âš ï¸ ì§ì ‘ ë§¤ì¹­ ì—†ìŒ (ìœ ì‚¬ í•­ëª©)\n\n"
    else:
        header = ""
    
    context = header + "\n\n" + "="*50 + "\n\n".join(relevant_chunks)
    pages_found = sorted(set(page_numbers))
    
    return context, pages_found


def search_relevant_chunks(
    query: str, 
    vector_db: Dict, 
    top_k: int = 10,
    min_similarity: float = 0.2
) -> Tuple[str, List[int]]:
    query_embedding = embedder.encode([query], device='cuda')
    similarities = np.dot(vector_db["embeddings"], query_embedding.T).flatten()
    
    valid_indices = np.where(similarities >= min_similarity)[0]
    
    if len(valid_indices) == 0:
        top_indices = np.argsort(similarities)[-min(5, len(similarities)):][::-1]
        return _format_chunks(vector_db, similarities, top_indices, fallback=True)
    
    top_k_valid = min(top_k, len(valid_indices))
    top_indices = valid_indices[np.argsort(similarities[valid_indices])[-top_k_valid:][::-1]]
    
    return _format_chunks(vector_db, similarities, top_indices)


def detect_and_remove_repetition(text: str, min_repeat: int = 3) -> str:
    lines = text.split('\n')
    seen_lines = {}
    clean_lines = []
    
    for line in lines:
        line_stripped = line.strip()
        if not line_stripped:
            clean_lines.append(line)
            continue
        
        if line_stripped in seen_lines:
            seen_lines[line_stripped] += 1
            if seen_lines[line_stripped] >= min_repeat:
                continue
        else:
            seen_lines[line_stripped] = 1
        
        clean_lines.append(line)
    
    text = '\n'.join(clean_lines)
    
    pattern = r'(.{20,})(\1{2,})'
    
    def replace_repetition(match):
        repeated_text = match.group(1)
        return repeated_text + " [ë°˜ë³µ ì œê±°]"
    
    text = re.sub(pattern, replace_repetition, text)
    
    return text


def validate_analysis_logic(analysis_text: str, vector_db: Optional[Dict] = None) -> Tuple[bool, List[Dict]]:
    issues = []

    pattern1 = re.finditer(r'ë‹µë³€:\s*âœ…\s*ì¶©ë¶„.*?ì˜í–¥ë„:\s*ğŸ”´\s*Critical', analysis_text, re.DOTALL | re.IGNORECASE)
    for match in pattern1:
        issues.append({
            "type": "ë…¼ë¦¬ì  ëª¨ìˆœ",
            "desc": "'ì¶©ë¶„'í•˜ë‹¤ê³  ë‹µí–ˆìœ¼ë‚˜ Criticalë¡œ í‰ê°€",
            "location": match.group()[:100]
        })

    pattern2 = re.finditer(r'(\d+%)\s*(ê°ì†Œ|ì¦ê°€|ì´ˆê³¼)', analysis_text)
    for match in pattern2:
        context = analysis_text[max(0, match.start()-200):match.end()+200]
        if 'p.' not in context and 'ë¬¸ì„œ' not in context and 'ì¶”ì •' not in context:
            issues.append({
                "type": "ê·¼ê±° ë¶€ì¡±",
                "desc": f"ì •ëŸ‰ì  í‘œí˜„ '{match.group()}' ì¶œì²˜ ë¯¸ëª…ì‹œ",
                "location": match.group()
            })

    # ğŸ†• í”Œë ˆì´ìŠ¤í™€ë” ê²€ì¦ ê°•í™”
    placeholders = re.findall(r'\[(í˜ì´ì§€|ê¸ˆì•¡|ì œëª©|êµ¬ì²´ì |ë‹´ë‹¹|ì¡°ì§|ë²ˆí˜¸|ì§ˆë¬¸|ì¸ìš©)\]', analysis_text)
    if placeholders:
        issues.append({
            "type": "ì¶œë ¥ ë¶ˆì™„ì „",
            "desc": f"í”Œë ˆì´ìŠ¤í™€ë” ë°œê²¬: {set(placeholders)}",
            "location": "multiple"
        })

    # ğŸ†• ì˜ˆì‹œ ë³µì‚¬ ê²€ì¦ (í˜•ì‹ ì˜ˆì‹œì— ìˆë˜ íŠ¹ì • ë‚´ìš© ê²€ì¶œ)
    example_keywords = [
        "íƒœì–‘ê´‘ ë°œì „ ì‹œìŠ¤í…œ",
        "ìš°ê¸° 4ê°œì›”",
        "ë””ì ¤ ë°œì „ê¸°",
        "í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ",
        "ì‹œë¯¼ë‹¨ì²´ X",
        "ì˜ˆì‚° ì¦ì•¡ 190ë§Œë¶ˆ",
        "1,060ë§Œë¶ˆì—ì„œ 1,250ë§Œë¶ˆ"
    ]

    copied_examples = [kw for kw in example_keywords if kw in analysis_text]
    if copied_examples:
        issues.append({
            "type": "âš ï¸ ì˜ˆì‹œ ë³µì‚¬ ì˜ì‹¬",
            "desc": f"í˜•ì‹ ì˜ˆì‹œ ë‚´ìš©ì´ ì¶œë ¥ì— í¬í•¨ë¨: {copied_examples[:3]}",
            "location": "multiple"
        })

    # ğŸ”¥ ë‹´ë‹¹ ê¸°ê´€ ê²€ì¦ (GIZ ê°™ì€ ì—‰ëš±í•œ ê¸°ê´€ ë°©ì§€)
    valid_orgs = ["KOICA", "GGGI", "MPI", "DPI", "DRI", "MONRE", "MoNRE", "MPWT", "DHUP", "DWCS", "DOT"]
    invalid_orgs = ["GIZ", "JICA", "USAID", "World Bank", "ADB", "UNDP"]

    for invalid_org in invalid_orgs:
        if invalid_org in analysis_text and "ë‹´ë‹¹" in analysis_text:
            # ë‹´ë‹¹ ê¸°ê´€ìœ¼ë¡œ ëª…ì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸
            pattern = re.search(rf'ë‹´ë‹¹[:\s]*{invalid_org}', analysis_text)
            if pattern:
                issues.append({
                    "type": "âš ï¸ ë‹´ë‹¹ ê¸°ê´€ ì˜¤ë¥˜",
                    "desc": f"'{invalid_org}'ëŠ” ë³¸ ì‚¬ì—…ì˜ ë‹´ë‹¹ ê¸°ê´€ì´ ì•„ë‹™ë‹ˆë‹¤ (KOICA/GGGI ì‚¬ì—…)",
                    "location": pattern.group()
                })

    # ğŸ”¥ ì¸ìš©ë¬¸ ê²€ì¦ (ë¹„í™œì„±í™” - ë„ˆë¬´ ì—„ê²©í•¨)
    # if vector_db:
    #     # p.[ìˆ«ì] "[ì¸ìš©ë¬¸]" íŒ¨í„´ ì°¾ê¸°
    #     citation_pattern = re.finditer(r'p\.(\d+)[^\n"]*?"([^"]{10,})"', analysis_text)
    #     for match in citation_pattern:
    #         page_num = int(match.group(1))
    #         quote = match.group(2)
    #
    #         # í•´ë‹¹ í˜ì´ì§€ì˜ ì²­í¬ì—ì„œ ì¸ìš©ë¬¸ ì°¾ê¸°
    #         page_chunks = [chunk for chunk in vector_db['chunks'] if chunk['page'] == page_num]
    #         found = any(quote[:20] in chunk['text'] or chunk['text'][:100] in quote for chunk in page_chunks)
    #
    #         if not found and len(page_chunks) > 0:
    #             issues.append({
    #                 "type": "âš ï¸ ì¸ìš©ë¬¸ ë¶ˆì¼ì¹˜",
    #                 "desc": f"p.{page_num}ì˜ ì¸ìš©ë¬¸ì´ ì‹¤ì œ ë¬¸ì„œì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ: \"{quote[:50]}...\"",
    #                 "location": match.group()[:80]
    #             })

    is_valid = len(issues) == 0
    return is_valid, issues


def comprehensive_post_processing(text: str, label: str) -> str:
    text = text.strip()
    
    unwanted_prefixes = [
        "Here is", "Sure,", "Certainly,", "Of course,",
        "I'll analyze", "Let me", "Based on the document",
        "According to", "The document shows"
    ]
    
    for prefix in unwanted_prefixes:
        if text.startswith(prefix):
            lines = text.split("\n")
            if len(lines) > 1:
                text = "\n".join(lines[1:]).strip()
            break
    
    lines = text.split("\n")
    if lines and (lines[0].startswith("##") or lines[0].startswith("**")):
        text = "\n".join(lines[1:]).strip()
    
    while "\n\n\n" in text:
        text = text.replace("\n\n\n", "\n\n")
    
    text = detect_and_remove_repetition(text)
    
    return text.strip()


# ==============================================
# Few-shot Examples (ğŸ”§ ë” ê°•ì¡°)
# ==============================================

ANALYSIS_EXAMPLES = """
# âš ï¸ ì¶œë ¥ í˜•ì‹ ê°€ì´ë“œ (í˜•ì‹ë§Œ ì°¸ê³ , ë‚´ìš©ì€ ì ˆëŒ€ ë³µì‚¬ ê¸ˆì§€!)

## í˜•ì‹ ì˜ˆì‹œ

### â“ ì§ˆë¬¸: [ë¬¸ì„œì—ì„œ ë°œê²¬í•œ ì‹¤ì œ ì´ìŠˆë¥¼ ì§ˆë¬¸ìœ¼ë¡œ ì‘ì„±]
- **ë‹µë³€**: âœ… ì¶©ë¶„ / âš ï¸ ë¶€ë¶„ì  / âŒ ì—†ìŒ
- **ê·¼ê±°**: p.[ì‹¤ì œ í˜ì´ì§€]ì—ì„œ "[ë¬¸ì„œì˜ ì‹¤ì œ ì¸ìš©ë¬¸]" ëª…ì‹œ
- **ë¬¸ì œì **:
  1) [ë¬¸ì„œì—ì„œ ë°œê²¬í•œ ì‹¤ì œ ë¬¸ì œì  1]
  2) [ë¬¸ì„œì—ì„œ ë°œê²¬í•œ ì‹¤ì œ ë¬¸ì œì  2]
  3) [ë¬¸ì„œì—ì„œ ë°œê²¬í•œ ì‹¤ì œ ë¬¸ì œì  3]
- **ì˜í–¥ë„**: ğŸ”´ Critical / ğŸŸ¡ High / ğŸŸ¢ Medium
- **ì˜ˆìƒ ì˜í–¥**: [êµ¬ì²´ì  ì‹œë‚˜ë¦¬ì˜¤, ê¸°ê°„, ê¸ˆì•¡]
- **ê¶Œê³ ì‚¬í•­**:
  1) ì¦‰ì‹œ/ë‹¨ê¸° ([ê¸°ê°„]): [êµ¬ì²´ì  ì¡°ì¹˜] - ì˜ˆì‚° [ê¸ˆì•¡] - ë‹´ë‹¹ [ì¡°ì§]
  2) ë‹¨ê¸°/ì¤‘ê¸° ([ê¸°ê°„]): [êµ¬ì²´ì  ì¡°ì¹˜] - ì˜ˆì‚° [ê¸ˆì•¡] - ë‹´ë‹¹ [ì¡°ì§]

---

ğŸš¨ **ê²½ê³ **:
- ìœ„ëŠ” í˜•ì‹ë§Œ ë³´ì—¬ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤
- ì°¸ê³  ë¬¸ì„œì—ì„œ ì‹¤ì œë¡œ ë°œê²¬í•œ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©(ì˜ˆ: íƒœì–‘ê´‘, ë””ì ¤, ì‹œë¯¼ë‹¨ì²´ ë“±)ì„ ì„ì˜ë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
"""

# ==============================================
# KOICA ì„¹í„° ì •ì˜ (v2.9ì™€ ë™ì¼)
# ==============================================

KOICA_SECTORS = {
    "êµìœ¡": {
        "keywords": ["êµìœ¡", "í•™êµ", "êµì‚¬", "í•™ìƒ", "êµê³¼", "í•™ìŠµ", "êµìœ¡ê³¼ì •", "literacy", "ëŒ€í•™", "ì§ì—…í›ˆë ¨"],
        "core_issues": ["êµìœ¡ ì ‘ê·¼ì„± ë° í˜•í‰ì„±", "êµìœ¡ í’ˆì§ˆ ë° í•™ìŠµ ì„±ê³¼", "êµì‚¬ ì—­ëŸ‰ ë° êµìœ¡ ì¸í”„ë¼", "êµìœ¡ê³¼ì • í˜„ì§€í™” ë° ì ì ˆì„±", "êµìœ¡ ê±°ë²„ë„ŒìŠ¤ ë° ì¬ì •"],
        "critical_questions": ["êµìœ¡ ì†Œì™¸ê³„ì¸µì˜ ì ‘ê·¼ì„±ì´ ë³´ì¥ë˜ëŠ”ê°€?", "í•™ìŠµ ì„±ê³¼ ì¸¡ì • ì²´ê³„ê°€ ìˆ˜ë¦½ë˜ì–´ ìˆëŠ”ê°€?", "í˜„ì§€ êµìœ¡ê³¼ì •ì´ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?", "êµì‚¬ ì–‘ì„± ê³„íšì´ ìˆëŠ”ê°€?", "ì‚¬ì—… ì¢…ë£Œ í›„ ì˜ˆì‚° í™•ë³´ ê³„íšì€?"]
    },
    "ë³´ê±´": {
        "keywords": ["ë³´ê±´", "ì˜ë£Œ", "ê±´ê°•", "ë³‘ì›", "í´ë¦¬ë‹‰", "ì§ˆë³‘", "ë°±ì‹ ", "health", "ì˜ì‚¬", "ê°„í˜¸ì‚¬", "í™˜ì"],
        "core_issues": ["ë³´ê±´ì˜ë£Œ ì ‘ê·¼ì„±", "ì˜ë£Œ ì„œë¹„ìŠ¤ ì§ˆ ë° ì•ˆì „", "ì£¼ìš” ì§ˆë³‘ ë¶€ë‹´", "ë³´ê±´ ì¸ë ¥ ë° ì¸í”„ë¼", "ë³´ê±´ ì‹œìŠ¤í…œ ê°•í™”"],
        "critical_questions": ["ì£¼ìš” ì§ˆë³‘ ë¶€ë‹´ì„ íŒŒì•…í–ˆëŠ”ê°€?", "ì˜ë£Œì¸ë ¥ í™•ë³´ ê³„íšì´ í˜„ì‹¤ì ì¸ê°€?", "ì˜ì•½í’ˆ ì§€ì† ê³µê¸‰ ë°©ì•ˆì€?", "ë³´ê±´ì •ë³´ì‹œìŠ¤í…œ ê³„íšì€?", "í˜„ì§€ ì‹œìŠ¤í…œ ì—°ê³„ëŠ”?"]
    },
    "ê±°ë²„ë„ŒìŠ¤Â·í‰í™”": {
        "keywords": ["ê±°ë²„ë„ŒìŠ¤", "í‰í™”", "ë²•", "ì œë„", "ë¯¼ì£¼", "ë¶€íŒ¨", "íˆ¬ëª…", "ë¶„ìŸ", "governance", "ì •ë¶€", "í–‰ì •"],
        "core_issues": ["ì •ë¶€ íš¨ê³¼ì„±", "ë¶€íŒ¨ í†µì œ", "ë²•ì¹˜", "ì‹œë¯¼ì‚¬íšŒ ì°¸ì—¬", "ë¶„ìŸ ì˜ˆë°©"],
        "critical_questions": ["ë¶€íŒ¨ ìœ„í—˜ í‰ê°€ê°€ ì„¤ê³„ë˜ì—ˆëŠ”ê°€?", "ì‹œë¯¼ ì°¸ì—¬ê°€ í¬í•¨ë˜ì—ˆëŠ”ê°€?", "ë²•ì œë„ ì‹¤í–‰ ê°€ëŠ¥ì„±ì€?", "ì •ì¹˜ ë¶ˆì•ˆì • ì˜í–¥ì€?", "ì¸ê¶Œ ê¸°ë°˜ ì ‘ê·¼ì´ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?"]
    },
    "ë†ì´Œê°œë°œ": {
        "keywords": ["ë†ì´Œ", "ë†ì—…", "ë†ë¯¼", "ë†ê°€", "ì‘ë¬¼", "ê°€ì¶•", "ì‹ëŸ‰", "rural", "agriculture", "ì˜ë†", "ìˆ˜í™•"],
        "core_issues": ["ë†ê°€ ì†Œë“ ì¦ëŒ€", "ì‹ëŸ‰ì•ˆë³´", "ë†ì—… ìƒì‚°ì„±", "ì‹œì¥ ì ‘ê·¼ì„±", "ê¸°í›„ë³€í™” ì ì‘"],
        "critical_questions": ["ì†Œë† ì¤‘ì‹¬ ì ‘ê·¼ì¸ê°€?", "ì‹œì¥ ì ‘ê·¼ì„±ì´ êµ¬ì²´ì ì¸ê°€?", "ê¸°í›„ ìŠ¤ë§ˆíŠ¸ ë†ì—…ì´ í¬í•¨ë˜ì—ˆëŠ”ê°€?", "í† ì§€ ê°ˆë“± ê°€ëŠ¥ì„±ì€?", "ë†ë¯¼ ì¡°ì§í™” ê³„íšì€?"]
    },
    "ë¬¼": {
        "keywords": ["ë¬¼", "ìˆ˜ìì›", "ìƒí•˜ìˆ˜ë„", "ìœ„ìƒ", "ì‹ìˆ˜", "water", "sanitation", "ì •ìˆ˜", "ë°°ìˆ˜"],
        "core_issues": ["ì•ˆì „í•œ ì‹ìˆ˜", "ìœ„ìƒì‹œì„¤", "ìˆ˜ìì› ê´€ë¦¬", "ìˆ˜ì§ˆ ëª¨ë‹ˆí„°ë§", "ë¬¼ ì•ˆë³´"],
        "critical_questions": ["ìˆ˜ì§ˆ ê²€ì‚¬ ì²´ê³„ëŠ”?", "ìœ ì§€ë³´ìˆ˜ ì¬ì›ì€?", "ìˆ˜ì¸ì„± ì§ˆë³‘ ëª©í‘œëŠ”?", "ì§€í•˜ìˆ˜ ì§€ì†ê°€ëŠ¥ì„±ì€?", "ì£¼ë¯¼ ì°¸ì—¬í˜• ê´€ë¦¬ëŠ”?"]
    },
    "ì—ë„ˆì§€": {
        "keywords": ["ì—ë„ˆì§€", "ì „ë ¥", "ë°œì „", "ì†¡ë°°ì „", "ì¬ìƒì—ë„ˆì§€", "íƒœì–‘ê´‘", "energy", "ì „ê¸°", "ë°œì „ì†Œ"],
        "core_issues": ["ì „ë ¥ ë³´ê¸‰ë¥ ", "ì „ë ¥ ì•ˆì •ì„±", "ì¬ìƒì—ë„ˆì§€ ì „í™˜", "ì—ë„ˆì§€ íš¨ìœ¨", "ì—ë„ˆì§€ ê±°ë²„ë„ŒìŠ¤"],
        "critical_questions": ["ì¬ìƒì—ë„ˆì§€ ëª©í‘œê°€ í˜„ì‹¤ì ì¸ê°€?", "ì „ë ¥ë§ ì—°ê³„ëŠ”?", "ì „ê¸°ìš”ê¸ˆ ì •ì±…ì€?", "ì—ë„ˆì§€ ë¹ˆê³¤ì¸µ ì§€ì›ì€?", "ê¸°ìˆ  ì ì •ì„±ì€?"]
    },
    "êµí†µ": {
        "keywords": ["êµí†µ", "ë„ë¡œ", "êµëŸ‰", "ìš´ì†¡", "ë¬¼ë¥˜", "transport", "road", "ê³ ì†ë„ë¡œ", "í•­ë§Œ"],
        "core_issues": ["êµí†µ ì ‘ê·¼ì„±", "êµí†µ ì•ˆì „", "ìœ ì§€ë³´ìˆ˜", "ë¬¼ë¥˜ íš¨ìœ¨ì„±", "í™˜ê²½ ì˜í–¥"],
        "critical_questions": ["ìœ ì§€ë³´ìˆ˜ ì¬ì›ì€?", "êµí†µì•ˆì „ ì‹œì„¤ì€?", "í™˜ê²½ì˜í–¥í‰ê°€ëŠ”?", "ê¸°í›„ ë¦¬ìŠ¤í¬ëŠ”?", "ì‹œì¥ ì ‘ê·¼ì„±ì€?"]
    },
    "ë„ì‹œ": {
        "keywords": ["ë„ì‹œ", "ì£¼ê±°", "ìŠ¬ëŸ¼", "ë„ì‹œê³„íš", "ìŠ¤ë§ˆíŠ¸ì‹œí‹°", "urban", "ì£¼íƒ", "ë„ì‹œê°œë°œ"],
        "core_issues": ["ë„ì‹œ ë¹ˆê³¤", "ë„ì‹œê³„íš", "ë„ì‹œ ì¸í”„ë¼", "ìŠ¤ë§ˆíŠ¸ì‹œí‹°", "ë„ì‹œ íšŒë³µë ¥"],
        "critical_questions": ["ê°•ì œ ì´ì£¼ ì—†ëŠ” ì ‘ê·¼ì¸ê°€?", "í¬ìš©ì  ê³„íšì¸ê°€?", "ê¸°ìˆ  ì ì •ì„±ì€?", "ì¬ë‚œ ëŒ€ì‘ì€?", "ë„ë† ì—°ê³„ëŠ”?"]
    },
    "ê³¼í•™ê¸°ìˆ í˜ì‹ ": {
        "keywords": ["ICT", "ë””ì§€í„¸", "í˜ì‹ ", "ê¸°ìˆ ", "ì—°êµ¬", "innovation", "technology", "ì˜ì‚¬", "consular", "ì •ë³´í†µì‹ ", "AI"],
        "core_issues": ["ë””ì§€í„¸ ê²©ì°¨", "ICT ì¸í”„ë¼", "ê¸°ìˆ  ì´ì „", "í˜ì‹  ìƒíƒœê³„", "ì‚¬ì´ë²„ ë³´ì•ˆ"],
        "critical_questions": ["ë””ì§€í„¸ ë¦¬í„°ëŸ¬ì‹œ êµìœ¡ì€?", "ì†”ë£¨ì…˜ ì„ íƒ íƒ€ë‹¹ì„±ì€?", "í˜„ì§€ ê¸°ìˆ  ì—­ëŸ‰ì€?", "ë°ì´í„° ë³´í˜¸ëŠ”?", "ê¸°ìˆ  ì¢…ì† ìœ„í—˜ì€?"]
    },
    "ê¸°í›„í–‰ë™": {
        "keywords": ["ê¸°í›„", "ì˜¨ì‹¤ê°€ìŠ¤", "íƒ„ì†Œ", "ì ì‘", "ì™„í™”", "climate", "í™˜ê²½", "ë°°ì¶œ"],
        "core_issues": ["ì˜¨ì‹¤ê°€ìŠ¤ ê°ì¶•", "ê¸°í›„ë³€í™” ì ì‘", "ê¸°í›„ ì¬ì›", "ê¸°í›„ íšŒë³µë ¥", "NDC ì´í–‰"],
        "critical_questions": ["ê°ì¶•ëŸ‰ ì¸¡ì • ê°€ëŠ¥í•œê°€?", "ì·¨ì•½ê³„ì¸µ ê³ ë ¤ëŠ”?", "ìì—°ê¸°ë°˜í•´ë²•ì€?", "NDC ì •í•©ì„±ì€?", "ì¥ê¸° ì‹œë‚˜ë¦¬ì˜¤ëŠ”?"]
    },
    "ì„±í‰ë“±": {
        "keywords": ["ì„±í‰ë“±", "ì  ë”", "ì—¬ì„±", "ì†Œë…€", "gender", "women", "ì—¬ì•„"],
        "core_issues": ["ì  ë” ê²©ì°¨", "ì—¬ì„± ì—­ëŸ‰ê°•í™”", "ì  ë” í­ë ¥ ì˜ˆë°©", "ì—¬ì„± ë¦¬ë”ì‹­", "ì  ë” ì£¼ë¥˜í™”"],
        "critical_questions": ["ì  ë” ë¶„ì„ì´ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?", "ì—¬ì„± ì°¸ì—¬ ëª©í‘œëŠ”?", "GBV ì˜ˆë°©ì€?", "ëŒë´„ ë¶€ë‹´ ê°ì†ŒëŠ”?", "ì  ë” ë°ì´í„°ëŠ”?"]
    },
    "ì¸ê¶Œ": {
        "keywords": ["ì¸ê¶Œ", "ì¥ì• ", "ì•„ë™", "ì†Œìˆ˜ì", "ì·¨ì•½ê³„ì¸µ", "human rights", "ê¶Œë¦¬"],
        "core_issues": ["ì¸ê¶Œ ê¸°ë°˜ ì ‘ê·¼", "ì‚¬íšŒì  ë°°ì œ", "ì·¨ì•½ê³„ì¸µ ë³´í˜¸", "ì•„ë™ê¶Œë¦¬", "ì¥ì•  í¬ìš©"],
        "critical_questions": ["Do No Harmì´ ì ìš©ë˜ì—ˆëŠ”ê°€?", "ì¥ì• ì¸ ì ‘ê·¼ì„±ì€?", "ì•„ë™ ë³´í˜¸ì •ì±…ì€?", "ì›ì£¼ë¯¼ ê¶Œë¦¬ëŠ”?", "ì¸ê¶Œ ì˜í–¥í‰ê°€ëŠ”?"]
    }
}

def detect_sector(text: str, extracted_info: str) -> Tuple[str, List[str]]:
    full_text = (text + extracted_info).lower()
    sector_scores = {}
    
    for sector, info in KOICA_SECTORS.items():
        score = 0
        matched_keywords = []
        
        for keyword in info["keywords"]:
            count = full_text.count(keyword.lower())
            if count > 0:
                score += count
                matched_keywords.append(f"{keyword}({count})")
        
        if score > 0:
            sector_scores[sector] = {"score": score, "keywords": matched_keywords}
    
    if not sector_scores:
        return "ì¼ë°˜", []
    
    sorted_sectors = sorted(sector_scores.items(), key=lambda x: x[1]["score"], reverse=True)
    primary_sector = sorted_sectors[0][0]
    primary_score = sorted_sectors[0][1]["score"]
    
    sectors = [primary_sector]
    
    if len(sorted_sectors) > 1:
        secondary_sector = sorted_sectors[1][0]
        secondary_score = sorted_sectors[1][1]["score"]
        
        if secondary_score >= primary_score * 0.5:
            sectors.append(secondary_sector)
    
    print(f"\nğŸ¯ ì„¹í„°: {', '.join(sectors)}")
    
    return sectors[0], sectors


# ==============================================
# TAG í”„ë¡¬í”„íŠ¸ (ğŸ”§ v3.0 ëŒ€í­ ê°œì„ )
# ==============================================

TAG_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ KOICA TAG ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# CRITICAL ê·œì¹™
1. **í”Œë ˆì´ìŠ¤í™€ë” ì ˆëŒ€ ê¸ˆì§€**: [ì§ˆë¬¸], [êµ¬ì²´ì ], [í˜ì´ì§€], [ê¸ˆì•¡], [ì¡°ì§] ë“± ëŒ€ê´„í˜¸ í˜•ì‹ ì‚¬ìš© ê¸ˆì§€
2. **ì‹¤ì œ ë‚´ìš© ì‘ì„±**: ëª¨ë“  ì¹¸ì„ ì‹¤ì œ ë¶„ì„ ë‚´ìš©ìœ¼ë¡œ ì±„ìš°ê¸°
3. **ë…¼ë¦¬ì  ì¼ê´€ì„±**: âœ…ì¶©ë¶„ â†’ ğŸ”´Critical ë¶ˆê°€
4. **ê·¼ê±° í•„ìˆ˜**: í˜ì´ì§€ ë²ˆí˜¸ + ì¸ìš© ë‚´ìš©

**ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ, ì‹¤ì œ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.**"""


# ğŸ”§ Agent 1 í”„ë¡¬í”„íŠ¸ ì™„ì „ ì¬ì‘ì„±
PROJECT_MANAGER_PROMPT = """ë‹¹ì‹ ì€ KOICA í”„ë¡œì íŠ¸ ê´€ë¦¬ ì „ë¬¸ê°€(PMC)ì…ë‹ˆë‹¤.

# ì—­í• 
ì‚¬ì—…ì˜ ë…¼ë¦¬ì„±, ì‹¤í–‰ ê°€ëŠ¥ì„±, ìœ„í—˜ì„ ê²€í† í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ ì•ˆ ì œì‹œ

# CRITICAL ì§€ì¹¨
- ì œê³µëœ ì˜ˆì‹œë¥¼ ì •í™•íˆ ë”°ë¼ ì‘ì„±
- [ì§ˆë¬¸], [êµ¬ì²´ì ], [í˜ì´ì§€] ê°™ì€ í”Œë ˆì´ìŠ¤í™€ë” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
- ëª¨ë“  ì§ˆë¬¸, ê·¼ê±°, ë¬¸ì œì , ê¶Œê³ ë¥¼ ì‹¤ì œ ë‚´ìš©ìœ¼ë¡œ ì±„ìš°ê¸°

# ì¶œë ¥ í˜•ì‹
ê° ì§ˆë¬¸:
- â“ ì§ˆë¬¸: [ì‹¤ì œ êµ¬ì²´ì  ì§ˆë¬¸ ì‘ì„±]
- ë‹µë³€: âœ…/âš ï¸/âŒ
- ê·¼ê±°: p.[ë²ˆí˜¸] "[ì‹¤ì œ ì¸ìš©]"
- ë¬¸ì œì : (3ê°œ, ì‹¤ì œ ë‚´ìš©)
- ì˜í–¥ë„: ğŸ”´/ğŸŸ¡/ğŸŸ¢
- ì˜ˆìƒ ì˜í–¥: (êµ¬ì²´ì  ê¸°ê°„/ê¸ˆì•¡)
- ê¶Œê³ ì‚¬í•­: (ì¦‰ì‹œ/ë‹¨ê¸°/ì¤‘ê¸°, ì‹¤ì œ ì˜ˆì‚°/ë‹´ë‹¹)"""


def get_sector_expert_prompt(sector: str) -> str:
    if sector not in KOICA_SECTORS:
        return TAG_SYSTEM_PROMPT

    sector_info = KOICA_SECTORS[sector]

    return f"""ë‹¹ì‹ ì€ KOICA {sector} ë¶„ì•¼ ìµœê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# ğŸ¯ ì „ë¬¸ ì—­í• 
- **ë¶„ì•¼**: {sector} ì„¹í„° êµ­ì œê°œë°œí˜‘ë ¥ ì „ë¬¸ê°€
- **ì„ë¬´**: ì‚¬ì—… ë¬¸ì„œë¥¼ **ì² ì €íˆ ê²€í† **í•˜ê³  **êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ** ê¶Œê³ ì‚¬í•­ ë„ì¶œ
- **ê¸°ì¤€**: êµ­ì œ ëª¨ë²” ì‚¬ë¡€, KOICA ê¸°ì¤€, SDGs ì •í•©ì„±

# ğŸ“‹ í•µì‹¬ ê²€í†  ì´ìŠˆ ({len(sector_info['core_issues'])}ê°œ)
{chr(10).join([f'{i+1}. **{issue}**' for i, issue in enumerate(sector_info['core_issues'])])}

# â“ í•„ìˆ˜ ê²€í†  ì§ˆë¬¸ ({len(sector_info['critical_questions'])}ê°œ)
{chr(10).join([f'{i+1}. {q}' for i, q in enumerate(sector_info['critical_questions'])])}

# ğŸ”¥ CRITICAL ë¶„ì„ ì›ì¹™

## 1ë‹¨ê³„: ì •í™•í•œ ë¬¸ì„œ ì´í•´ (ìµœìš°ì„ )
âš ï¸ **ì´ ë¬¸ì„œëŠ” ì‚¬ì—… "ê³„íšì„œ"ì…ë‹ˆë‹¤** (ì™„ë£Œëœ ì‚¬ì—… ë³´ê³ ì„œê°€ ì•„ë‹˜)
- âœ… **"~í•  ê²ƒì´ë‹¤" / "~í•  ì˜ˆì •ì´ë‹¤" = ì‚¬ì—…ì˜ ëª©í‘œ ë° ê³„íš** (ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤!)
- âœ… **"ê³„íšëœ ë‚´ìš©"ê³¼ "ëˆ„ë½ëœ ë‚´ìš©"ì„ ëª…í™•íˆ êµ¬ë¶„**í•˜ì„¸ìš”
- âœ… ì‚¬ì—…ì´ **ì´ë¯¸ ë‹¬ì„±í•œ ê²ƒ**ê³¼ **ì•ìœ¼ë¡œ ë‹¬ì„±í•  ê²ƒ**ì„ êµ¬ë¶„í•˜ì„¸ìš”
- âŒ ê³„íšì„œì— "~í•  ê²ƒì´ë‹¤"ë¼ê³  ì íŒ ë‚´ìš©ì„ "ì•„ì§ ì•ˆ ë˜ì–´ ìˆë‹¤"ëŠ” ë¬¸ì œë¡œ í•´ì„í•˜ì§€ ë§ˆì„¸ìš”

## 2ë‹¨ê³„: ìœ„í—˜ê´€ë¦¬ ì¤‘ì‹¬ ê²€í†  (ì¡°ë ¥ì ì—­í• )
ë‹¹ì‹ ì€ **ë¹„íŒìê°€ ì•„ë‹Œ ì¡°ë ¥ì**ì…ë‹ˆë‹¤. ë‹¤ìŒ 3ê°€ì§€ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

[1ë‹¨ê³„] **ì´ ì‚¬ì—… ê³„íšì˜ ê°•ì ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?**
   - ì˜ ì„¤ê³„ëœ ë¶€ë¶„, êµ­ì œ ëª¨ë²” ì‚¬ë¡€ ë°˜ì˜, í˜ì‹ ì  ì ‘ê·¼ë²• ë“±
   - **ì •ëŸ‰ì  ë°ì´í„°** í™œìš© (%, ê¸ˆì•¡, ì¸ì›, ê¸°ê°„ ë“±)

[2ë‹¨ê³„] **ì´ ì‚¬ì—…ì´ ì„±ê³µí•˜ëŠ” ë° ë°©í•´ê°€ ë  ì ì¬ì  ìœ„í—˜(Risk)ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?**
   - ë…¼ë¦¬ì  ì¼ê´€ì„± ë¶€ì¡±, ì‹¤í–‰ ê°€ëŠ¥ì„± ì˜ë¬¸, ëˆ„ë½ëœ ì¤‘ìš” ì‚¬í•­ ë“±
   - **ìœ„í—˜ì˜ ì˜í–¥ë„** í‰ê°€ (Critical / High / Medium)

[3ë‹¨ê³„] **ê° ìœ„í—˜ì„ ì˜ˆë°©(Mitigate)í•˜ê¸° ìœ„í•œ êµ¬ì²´ì  ì¡°ì¹˜ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?**
   - ì¦‰ì‹œ ì¡°ì¹˜ + ë‹¨ê¸° ì¡°ì¹˜ ì œì‹œ
   - ì¡°ì¹˜ë§ˆë‹¤ **ì˜ˆì‚° ê·œëª¨, ë‹´ë‹¹ ê¸°ê´€, ì‹¤í–‰ ê¸°ê°„** ëª…ì‹œ
   - **ì¸¡ì • ê°€ëŠ¥í•œ ê°œì„  ëª©í‘œ** ì„¤ì • (ì˜ˆ: "ì ‘ê·¼ì„± 30% í–¥ìƒ", "ë¹„ìš© 20% ì ˆê°")

## ì ˆëŒ€ ê¸ˆì§€
- [ì§ˆë¬¸], [êµ¬ì²´ì ], [í˜ì´ì§€], [ê¸ˆì•¡], [ì¡°ì§] ê°™ì€ í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©
- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš© ì„ì˜ë¡œ ë§Œë“¤ê¸°
- í˜•ì‹ ì˜ˆì‹œì˜ ë‚´ìš©(íƒœì–‘ê´‘, ë””ì ¤ ë“±) ë³µì‚¬
- ê·¼ê±° ì—†ëŠ” í‰ê°€ (ë°˜ë“œì‹œ í˜ì´ì§€ ë²ˆí˜¸ + ì¸ìš©ë¬¸ í¬í•¨)

## í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- ëª¨ë“  ì´ìŠˆì™€ ì§ˆë¬¸ì— ëŒ€í•´ **ê¶Œê³ ì‚¬í•­ í•„ìˆ˜** ì‘ì„±
- **ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ + ì‹¤ì œ ì¸ìš©ë¬¸** ë°˜ë“œì‹œ í¬í•¨
- ë…¼ë¦¬ì  ì¼ê´€ì„± ìœ ì§€ (âœ…ì¶©ë¶„ â†’ ğŸ”´Critical ë¶ˆê°€)
- ì„¹í„°ë³„ êµ­ì œ í‘œì¤€ ë° ëª¨ë²” ì‚¬ë¡€ ì–¸ê¸‰

**ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ, ì‹¤ì œ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.**"""


# ==============================================
# ë¶„ì„ í•¨ìˆ˜ë“¤ (ğŸ”§ í”„ë¡¬í”„íŠ¸ ì™„ì „ ì¬ì‘ì„±)
# ==============================================

@track_time
def extract_key_info_rag(full_text: str, vector_db: Dict) -> str:
    context, pages = search_relevant_chunks(
        "ì‚¬ì—…ëª… ê¸°ê°„ ì˜ˆì‚° ëª©í‘œ ì„±ê³¼ì§€í‘œ", 
        vector_db, 
        top_k=10
    )
    
    user_prompt = f"""ì°¸ê³  ë¬¸ì„œ (p.{', '.join(map(str, pages))}):
{context[:4000]}

---

ìœ„ ë¬¸ì„œì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

## ì‚¬ì—… ê¸°ë³¸ì •ë³´
- **ì‚¬ì—…ëª…**: [ì‹¤ì œ ì‚¬ì—…ëª…]
- **ê¸°ê°„**: [ì‹¤ì œ ê¸°ê°„]
- **ì´ ì˜ˆì‚°**: [ì‹¤ì œ ê¸ˆì•¡]
- **ì‚¬ì—… ëª©í‘œ**: [ì‹¤ì œ ëª©í‘œ]
- **í˜‘ë ¥ê¸°ê´€**: [ì‹¤ì œ ê¸°ê´€ëª…]

## ì£¼ìš” í™œë™ (5ê°œ)
1. [ì‹¤ì œ í™œë™ 1]
2. [ì‹¤ì œ í™œë™ 2]
...

ì •ë³´ê°€ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ í™•ì¸ ë¶ˆê°€"."""
    
    response = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": TAG_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ],
        max_tokens=3000,
        temperature=0.3,      # Mistral ìµœì í™”
        top_p=0.95,
        top_k=50,
        repeat_penalty=1.1
    )
    
    output = response['choices'][0]['message']['content']
    return comprehensive_post_processing(output, "ì •ë³´ì¶”ì¶œ")


@track_time
def multi_agent_analysis(vector_db: Dict, extracted_info: str, text: str) -> Tuple[str, str, List[str]]:
    """ì„¹í„° ì „ë¬¸ê°€ ì§‘ì¤‘ ë¶„ì„ (PMC ì œê±°, ì„¹í„° ì „ë¬¸ì„± ê°•í™”)"""

    primary_sector, all_sectors = detect_sector(text, extracted_info)

    print(f"\nğŸ¯ ì„¹í„° ì „ë¬¸ê°€ ë¶„ì„")
    print(f"  - ì£¼ì„¹í„°: {primary_sector}")
    if len(all_sectors) > 1:
        print(f"  - ë¶€ì„¹í„°: {', '.join(all_sectors[1:])}")

    # ì„¹í„° ì „ë¬¸ê°€ ì§‘ì¤‘ ë¶„ì„
    print(f"\nğŸ‘¤ {primary_sector} ì „ë¬¸ê°€ ë¶„ì„ ì¤‘...")

    if primary_sector in KOICA_SECTORS:
        sector_info = KOICA_SECTORS[primary_sector]

        # ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (top_k ìµœì í™”)
        sector_keywords = " ".join(sector_info["keywords"][:10])
        context, pages = search_relevant_chunks(sector_keywords, vector_db, top_k=10)

        sector_expert_prompt = get_sector_expert_prompt(primary_sector)

        user_prompt = f"""**ì„¹í„°**: {primary_sector}

**ì‚¬ì—… ì •ë³´**:
{extracted_info[:1000]}

**ì°¸ê³  ë¬¸ì„œ** (p.{', '.join(map(str, pages))}):
{context[:3500]}

---

{ANALYSIS_EXAMPLES}

---

ğŸ¯ **ê³¼ì œ**: {primary_sector} ë¶„ì•¼ ì „ë¬¸ê°€ë¡œì„œ ì•„ë˜ í•µì‹¬ ì´ìŠˆì™€ í•„ìˆ˜ ì§ˆë¬¸ì„ **ìœ„í—˜ê´€ë¦¬ ê´€ì **ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”.

âš ï¸ **ì¤‘ìš”**: ì´ ë¬¸ì„œëŠ” "ì‚¬ì—… ê³„íšì„œ"ì…ë‹ˆë‹¤. "~í•  ê²ƒì´ë‹¤"ëŠ” ëª©í‘œì´ì§€ ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤!

## ğŸ“‹ í•µì‹¬ ì´ìŠˆ ê²€í†  ({len(sector_info['core_issues'])}ê°œ)

{chr(10).join([f'### ì´ìŠˆ {i+1}: {issue}' for i, issue in enumerate(sector_info['core_issues'])])}

**ê° ì´ìŠˆë³„ë¡œ ë‹¤ìŒ 3ë‹¨ê³„ë¡œ ì‘ì„±**:

### [1ë‹¨ê³„] ê°•ì  íŒŒì•…
- **í˜„í™©**: ë¬¸ì„œì—ì„œ ë°œê²¬í•œ ì‹¤ì œ ë‚´ìš© (í˜ì´ì§€ ë²ˆí˜¸ + ì¸ìš©, ì—†ìœ¼ë©´ "ê´€ë ¨ ë‚´ìš© ë¯¸ë°œê²¬")
- **ê°•ì **: ì´ ê³„íšì—ì„œ ì˜ ì„¤ê³„ëœ ë¶€ë¶„ (êµ­ì œ ëª¨ë²” ì‚¬ë¡€, í˜ì‹ ì  ì ‘ê·¼ ë“±)
- **í‰ê°€**: ìš°ìˆ˜ / ë³´í†µ / ë¯¸í¡

### [2ë‹¨ê³„] ìœ„í—˜ ìš”ì¸ íŒŒì•…
- **ìœ„í—˜**: ì‚¬ì—… ì„±ê³µì„ ë°©í•´í•  ì ì¬ì  ìœ„í—˜ ìš”ì¸ (3ê°œ)
- **ì˜í–¥ë„**: Critical / High / Medium
- **ì˜ˆìƒ ì˜í–¥**: êµ¬ì²´ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ (ê¸°ê°„, ê¸ˆì•¡, ë²”ìœ„)

### [3ë‹¨ê³„] ìœ„í—˜ ì˜ˆë°© ì¡°ì¹˜
- **ì¦‰ì‹œ ì¡°ì¹˜**: [êµ¬ì²´ì  ì¡°ì¹˜] - ì˜ˆì‚°: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"] - ë‹´ë‹¹: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ì‚¬ì—…ë‹¨ í˜‘ì˜"] - ê¸°ê°„: [Xì£¼/ê°œì›”]
- **ë‹¨ê¸° ì¡°ì¹˜**: [êµ¬ì²´ì  ì¡°ì¹˜] - ì˜ˆì‚°: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"] - ë‹´ë‹¹: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ì‚¬ì—…ë‹¨ í˜‘ì˜"] - ê¸°ê°„: [Xê°œì›”]

---

## â“ í•„ìˆ˜ ì§ˆë¬¸ ê²€í†  ({len(sector_info['critical_questions'])}ê°œ)

{chr(10).join([f'{i+1}. {q}' for i, q in enumerate(sector_info['critical_questions'])])}

**ê° ì§ˆë¬¸ë³„ë¡œ ë‹¤ìŒ 3ë‹¨ê³„ë¡œ ì‘ì„±**:

### [1ë‹¨ê³„] ê³„íš ë‚´ìš© í™•ì¸
- **ë‹µë³€**: ì¶©ë¶„ / ë¶€ë¶„ì  / ì—†ìŒ
- **ê·¼ê±°**: [ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì€ ê²½ìš° p.Xì—ì„œ ì¸ìš©, ì°¾ì§€ ëª»í•œ ê²½ìš° "ë¬¸ì„œì—ì„œ ì§ì ‘ì ì¸ ì–¸ê¸‰ ì—†ìŒ"]

### [2ë‹¨ê³„] ìœ„í—˜ ìš”ì¸
- **ìœ„í—˜**: ì´ ë¶€ë¶„ì—ì„œ ë°œê²¬í•œ ì ì¬ì  ìœ„í—˜ (3ê°œ)
- **ì˜í–¥ë„**: Critical / High / Medium

### [3ë‹¨ê³„] ì˜ˆë°© ì¡°ì¹˜
- **ê¶Œê³ ì‚¬í•­**: ì¦‰ì‹œ/ë‹¨ê¸°/ì¤‘ê¸° ì¡°ì¹˜ - ì˜ˆì‚°: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"] - ë‹´ë‹¹: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ì‚¬ì—…ë‹¨ í˜‘ì˜"] - ê¸°ê°„: [Xê°œì›”]

---

ğŸš¨ **ì ˆëŒ€ ê¸ˆì§€**:
- í˜•ì‹ ì˜ˆì‹œì˜ ë‚´ìš©(íƒœì–‘ê´‘, ë””ì ¤, ì˜ˆì‚° ì¦ì•¡ 190ë§Œë¶ˆ ë“±) ë³µì‚¬ ê¸ˆì§€
- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì„ì˜ë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
- [ì§ˆë¬¸], [êµ¬ì²´ì ], [í˜ì´ì§€] ê°™ì€ í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš© ê¸ˆì§€
- **ì˜ˆì‚° ë‚ ì¡° ê¸ˆì§€**: ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ êµ¬ì²´ì  ê¸ˆì•¡(50ë§Œë¶ˆ, 100ë§Œë¶ˆ ë“±)ì„ ì„ì˜ë¡œ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”
- **ê·¼ê±° ë‚ ì¡° ê¸ˆì§€**: ë‚´ìš©ê³¼ ë¬´ê´€í•œ í˜ì´ì§€ë¥¼ ì¸ìš©í•˜ì§€ ë§ˆì„¸ìš” (ì˜ˆ: ì•ˆì „í•œ ì‹ìˆ˜ ë¶„ì„ì— íê¸°ë¬¼ í˜ì´ì§€ ì¸ìš©)

âœ… **í•„ìˆ˜**:
- ì°¸ê³  ë¬¸ì„œì—ì„œ ì‹¤ì œë¡œ ë°œê²¬í•œ ë‚´ìš©ë§Œ ì‚¬ìš©
- ê·¼ê±°ê°€ ë¶ˆí™•ì‹¤í•˜ë©´ "ë¬¸ì„œì—ì„œ ì§ì ‘ì ì¸ ì–¸ê¸‰ ì—†ìŒ"ìœ¼ë¡œ ëª…ì‹œ
- ì˜ˆì‚°ì´ ë¬¸ì„œì— ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"ë¡œ ëª…ì‹œ
- ë‹´ë‹¹ ê¸°ê´€ì´ ë¬¸ì„œì— ì—†ìœ¼ë©´ "ì‚¬ì—…ë‹¨ í˜‘ì˜"ë¡œ ëª…ì‹œ
- ëª¨ë“  ì´ìŠˆì™€ ì§ˆë¬¸ì— ëŒ€í•´ ê¶Œê³ ì‚¬í•­ í•„ìˆ˜ ì‘ì„±
- ì •ëŸ‰ì  ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í™œìš© (%, ê¸ˆì•¡, ì¸ì› ë“±)"""

        # ê²€ì¦ + ì¬ìƒì„± ë£¨í”„ ì‚¬ìš© (ì˜¤ë¥˜ ë°œê²¬ ì‹œ ìë™ ì¬ìƒì„±)
        sector_analysis = generate_with_validation(
            messages=[
                {"role": "system", "content": sector_expert_prompt},
                {"role": "user", "content": user_prompt}
            ],
            vector_db=vector_db,
            max_retries=2,
            max_tokens=6000
        )

    else:
        sector_analysis = f"## {primary_sector} ë¶„ì•¼\n\nì¼ë°˜ ë¶„ì•¼ë¡œ ì„¹í„° íŠ¹í™” ë¶„ì„ ìƒëµ."

    full_analysis = f"""# ğŸ¯ {primary_sector} ì„¹í„° ì „ë¬¸ê°€ TAG ë¶„ì„

**ë¶„ì„ ì²´ê³„**: {primary_sector} ì „ë¬¸ê°€ ì§‘ì¤‘ ê²€í† 
**ê²€í†  ì´ìŠˆ**: {len(KOICA_SECTORS.get(primary_sector, {}).get('core_issues', []))}ê°œ
**í•„ìˆ˜ ì§ˆë¬¸**: {len(KOICA_SECTORS.get(primary_sector, {}).get('critical_questions', []))}ê°œ

---

{sector_analysis}
"""

    return full_analysis, primary_sector, all_sectors


@track_time
def multi_agent_recommendations(vector_db: Dict, extracted_info: str, analysis: str, sector: str) -> str:
    """ì„¹í„° ì „ë¬¸ê°€ í†µí•© ê¶Œê³ ì•ˆ"""

    context, pages = search_relevant_chunks("ê°œì„  ê¶Œê³  ì¡°ì¹˜", vector_db, top_k=10)

    user_prompt = f"""**ì„¹í„°**: {sector}

**{sector} ì „ë¬¸ê°€ ë¶„ì„ ìš”ì•½**:
{analysis[:3500]}

**ì°¸ê³  ë¬¸ì„œ** (p.{', '.join(map(str, pages))}):
{context[:2500]}

---

ğŸ¯ **ê³¼ì œ**: {sector} ë¶„ì•¼ ì „ë¬¸ê°€ë¡œì„œ ìœ„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ **ì‹¤í–‰ ê°€ëŠ¥í•œ í†µí•© ê¶Œê³ ì•ˆ**ì„ ì‘ì„±í•˜ì„¸ìš”.

âš ï¸ **ì¤‘ìš”**: ì´ ë¬¸ì„œëŠ” "ì‚¬ì—… ê³„íšì„œ"ì…ë‹ˆë‹¤. "~í•  ê²ƒì´ë‹¤"ëŠ” ëª©í‘œì´ì§€ ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤!

## [Critical] ìš°ì„ ìˆœìœ„ ìœ„í—˜ (3ê°œ)

ê° ìœ„í—˜ë³„ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:

### ìœ„í—˜ 1: [ì„¹í„° ê´€ì ì˜ êµ¬ì²´ì  ì œëª©]
- **ë¶„ì•¼**: {sector}
- **ìœ„í—˜**: [100ì ì´ë‚´ë¡œ í•µì‹¬ ìœ„í—˜ ê¸°ìˆ ]
- **ê·¼ê±°**: [ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì€ ê²½ìš° p.Xì—ì„œ ì¸ìš©, ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ ì§ì ‘ì ì¸ ì–¸ê¸‰ ì—†ìŒ"]
- **ì˜í–¥**: [êµ¬ì²´ì  ì‹œë‚˜ë¦¬ì˜¤ - ëˆ„ê°€, ì–¸ì œ, ì–´ë–»ê²Œ ì˜í–¥ë°›ëŠ”ì§€]
- **ì¦‰ì‹œ ì¡°ì¹˜**: [ì¡°ì¹˜ ë‚´ìš©] - ì˜ˆì‚°: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"] - ë‹´ë‹¹: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ì‚¬ì—…ë‹¨ í˜‘ì˜"] - ê¸°ê°„: [Xì£¼/ê°œì›”]
- **ê¸°ëŒ€íš¨ê³¼**: [ì¸¡ì • ê°€ëŠ¥í•œ ê°œì„  ëª©í‘œ]

### ìœ„í—˜ 2, 3: [ìœ„ì™€ ë™ì¼í•œ í˜•ì‹]

---

## [High] ìš°ì„ ìˆœìœ„ ìœ„í—˜ (3ê°œ)

ê° ìœ„í—˜ë³„ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:

### ìœ„í—˜ 4: [êµ¬ì²´ì  ì œëª©]
- **ìœ„í—˜**: [80ì ì´ë‚´]
- **ê·¼ê±°**: [ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì€ ê²½ìš° p.Xì—ì„œ ì¸ìš©, ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ ì§ì ‘ì ì¸ ì–¸ê¸‰ ì—†ìŒ"]
- **ë‹¨ê¸° ì¡°ì¹˜**: [ì¡°ì¹˜] - ì˜ˆì‚°: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"] - ë‹´ë‹¹: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ì‚¬ì—…ë‹¨ í˜‘ì˜"] - ê¸°ê°„: [Xê°œì›”]
- **íš¨ê³¼**: [ì •ëŸ‰ì  ëª©í‘œ]

### ìœ„í—˜ 5, 6: [ìœ„ì™€ ë™ì¼í•œ í˜•ì‹]

---

## {sector} ì „ë¬¸ê°€ ì¢…í•© ì˜ê²¬

### í•µì‹¬ ë©”ì‹œì§€ (3ì¤„)
1. [ì„¹í„° ê´€ì ì˜ í•µì‹¬ ë©”ì‹œì§€ 1]
2. [ì„¹í„° ê´€ì ì˜ í•µì‹¬ ë©”ì‹œì§€ 2]
3. [ì„¹í„° ê´€ì ì˜ í•µì‹¬ ë©”ì‹œì§€ 3]

### ë¬¸ì„œ í’ˆì§ˆ í‰ê°€
- **ì ìˆ˜**: [X]/100ì 
- **ê°•ì **: [2ê°œ]
- **ì•½ì **: [3ê°œ]
- **ê°œì„  í•„ìš”**: [ìš°ì„ ìˆœìœ„ 3ê°œ]

### ìµœìš°ì„  ì¡°ì¹˜ (3ê°œ)
1. **[ì¡°ì¹˜ëª…]** - ê¸°ê°„: [Xì£¼/ê°œì›”] - ì˜ˆì‚°: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"] - ì´ìœ : [ì™œ ìµœìš°ì„ ì¸ì§€ 1ì¤„ ì„¤ëª…]
2. **[ì¡°ì¹˜ëª…]** - ê¸°ê°„: [Xì£¼/ê°œì›”] - ì˜ˆì‚°: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"] - ì´ìœ : [1ì¤„ ì„¤ëª…]
3. **[ì¡°ì¹˜ëª…]** - ê¸°ê°„: [Xì£¼/ê°œì›”] - ì˜ˆì‚°: [ë¬¸ì„œ ëª…ì‹œ ì‹œ ê¸°ì¬, ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"] - ì´ìœ : [1ì¤„ ì„¤ëª…]

### {sector} ì„¹í„° êµ­ì œ ê¸°ì¤€ ë° ëª¨ë²” ì‚¬ë¡€
- [í•´ë‹¹ ì„¹í„°ì˜ êµ­ì œ í‘œì¤€, SDGs ëª©í‘œ, ëª¨ë²” ì‚¬ë¡€ ë“± ì–¸ê¸‰]
- [ì´ ì‚¬ì—…ì´ êµ­ì œ ê¸°ì¤€ê³¼ ì–´ë–»ê²Œ ë¶€í•©/ë¶ˆì¼ì¹˜í•˜ëŠ”ì§€]

---

**ì ˆëŒ€ ê¸ˆì§€**:
- [ì§ˆë¬¸], [êµ¬ì²´ì ], [í˜ì´ì§€], [ê¸ˆì•¡] ë“± í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©
- ê·¼ê±° ì—†ëŠ” ì£¼ì¥
- í˜•ì‹ ì˜ˆì‹œ ë‚´ìš© ë³µì‚¬
- **ì˜ˆì‚° ë‚ ì¡° ê¸ˆì§€**: ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ êµ¬ì²´ì  ê¸ˆì•¡(50ë§Œë¶ˆ, 100ë§Œë¶ˆ ë“±)ì„ ì„ì˜ë¡œ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”
- **ê·¼ê±° ë‚ ì¡° ê¸ˆì§€**: ë‚´ìš©ê³¼ ë¬´ê´€í•œ í˜ì´ì§€ë¥¼ ì¸ìš©í•˜ì§€ ë§ˆì„¸ìš”

**í•„ìˆ˜**:
- ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©
- ê·¼ê±°ê°€ ë¶ˆí™•ì‹¤í•˜ë©´ "ë¬¸ì„œì—ì„œ ì§ì ‘ì ì¸ ì–¸ê¸‰ ì—†ìŒ"ìœ¼ë¡œ ëª…ì‹œ
- ì˜ˆì‚°ì´ ë¬¸ì„œì— ì—†ìœ¼ë©´ "ë³„ë„ ì‚°ì • í•„ìš”"ë¡œ ëª…ì‹œ
- ë‹´ë‹¹ ê¸°ê´€ì´ ë¬¸ì„œì— ì—†ìœ¼ë©´ "ì‚¬ì—…ë‹¨ í˜‘ì˜"ë¡œ ëª…ì‹œ
- ì¸¡ì • ê°€ëŠ¥í•œ ëª©í‘œ ì„¤ì •
- {sector} ì„¹í„° ì „ë¬¸ì„± ë°˜ì˜"""

    # ê²€ì¦ + ì¬ìƒì„± ë£¨í”„ ì‚¬ìš© (ì˜¤ë¥˜ ë°œê²¬ ì‹œ ìë™ ì¬ìƒì„±)
    output = generate_with_validation(
        messages=[
            {"role": "system", "content": get_sector_expert_prompt(sector)},
            {"role": "user", "content": user_prompt}
        ],
        vector_db=vector_db,
        max_retries=2,
        max_tokens=6000
    )

    return output


# ==============================================
# ë©”ì¸ í•¨ìˆ˜, UI (v2.9ì™€ ìœ ì‚¬)
# ==============================================

def upload_and_analyze_rag(pdf_file, progress=gr.Progress()):
    vector_db = None
    
    try:
        if pdf_file is None:
            yield "âŒ PDF ì—…ë¡œë“œ í•„ìš”", "", "", "", ""
            return
        
        progress(0, desc="ğŸ“„ PDF...")
        try:
            with pdfplumber.open(pdf_file.name) as pdf:
                total_pages = len(pdf.pages)
                if total_pages == 0:
                    yield "âŒ ë¹ˆ PDF", "", "", "", ""
                    return
                text = "".join(page.extract_text() or "" for page in pdf.pages)
                if len(text) < 500:
                    yield "âŒ í…ìŠ¤íŠ¸ ë¶€ì¡±", "", "", "", ""
                    return
        except Exception as e:
            yield f"âŒ PDF ì‹¤íŒ¨: {str(e)}", "", "", "", ""
            return
        
        filename = pdf_file.name.split('/')[-1]
        status = f"âœ… {filename}\nğŸ“„ {total_pages}p"
        yield status, "", "", "", ""
        
        progress(0.1, desc="ğŸ” ì¸ë±ì‹±...")
        try:
            chunks = chunk_text(text)
            vector_db = create_vector_db(chunks)
            
            rag_info = f"""## ğŸ—„ï¸ ë¬¸ì„œ ì •ë³´

**ë¬¸ì„œ**: {total_pages}p, {len(text):,}ì
**ì²­í¬**: {len(chunks)}ê°œ
**ì‹œìŠ¤í…œ**: TAG v4.0 (ì„¹í„° ì „ë¬¸ê°€ ì§‘ì¤‘)

ğŸ”¥ **v4.0 ì£¼ìš” ë³€ê²½**:
- PMC ë¶„ì„ ì œê±° (Agent 6íšŒ â†’ 1íšŒë¡œ ì¶•ì†Œ)
- ì„¹í„° ì „ë¬¸ê°€ ë¶„ì„ë§Œ ì§‘ì¤‘ (ë¹¡ì„¼ ê²€í† )
- LLM í˜¸ì¶œ ëŒ€í­ ê°ì†Œ (ì†ë„ í–¥ìƒ)
- ì„¹í„°ë³„ í•µì‹¬ ì´ìŠˆ + í•„ìˆ˜ ì§ˆë¬¸ ê°•í™”

âœ… ì¸ë±ì‹± ì™„ë£Œ!"""
            
            yield status, rag_info, "", "", ""
        except Exception as e:
            yield status, f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}", "", "", ""
            return
        
        step1 = ""
        try:
            progress(0.2, desc="1ï¸âƒ£ ì •ë³´...")
            step1 = extract_key_info_rag(text, vector_db)
            yield status, rag_info, step1, "", ""
        except Exception as e:
            step1 = f"âŒ 1ë‹¨ê³„ ì‹¤íŒ¨: {str(e)}"
            yield status, rag_info, step1, "", ""
        
        step2 = ""
        detected_sector = "ì¼ë°˜"
        try:
            progress(0.4, desc="2ï¸âƒ£ ë¶„ì„...")
            step2, detected_sector, all_sectors = multi_agent_analysis(vector_db, step1, text)
            
            rag_info += f"\n\n## ğŸ¯ ì„¹í„°\n- **{detected_sector}**"
            
            yield status, rag_info, step1, step2, ""
        except Exception as e:
            step2 = f"âŒ 2ë‹¨ê³„ ì‹¤íŒ¨: {str(e)}"
            yield status, rag_info, step1, step2, ""
        
        step3 = ""
        try:
            progress(0.75, desc="3ï¸âƒ£ ê¶Œê³ ...")
            step3 = multi_agent_recommendations(vector_db, step1, step2, detected_sector)
        except Exception as e:
            step3 = f"âŒ 3ë‹¨ê³„ ì‹¤íŒ¨: {str(e)}"
        
        progress(1.0, desc="âœ… ì™„ë£Œ!")
        
        timing_summary = "\n".join([f"  - {k}: {sum(v):.1f}ì´ˆ" for k, v in timing_stats.items()])
        
        final_status = f"""{status}

ğŸ‰ ì„¹í„° ì „ë¬¸ê°€ ë¶„ì„ ì™„ë£Œ!

ğŸ¯ ì„¹í„°: {detected_sector}

â±ï¸ ì‹œê°„:
{timing_summary}

ğŸ”¥ v4.0:
  âœ… PMC ì œê±° (LLM 6íšŒâ†’1íšŒ)
  âœ… ì„¹í„° ì „ë¬¸ê°€ ì§‘ì¤‘
  âœ… ë¹¡ì„¼ ê²€í†  ê°•í™”
  âœ… ì²˜ë¦¬ ì†ë„ ëŒ€í­ í–¥ìƒ"""
        
        yield final_status, rag_info, step1, step2, step3
        
    except Exception as e:
        yield f"âŒ ì˜¤ë¥˜: {str(e)}", "", "", "", ""
    
    finally:
        if vector_db:
            del vector_db
        torch.cuda.empty_cache()
        gc.collect()


def generate_clean_report(rag, info, analysis, recs):
    report = f"""{'='*80}
KOICA TAG v4.0 ì„¹í„° ì „ë¬¸ê°€ ë¶„ì„ ë³´ê³ ì„œ
{'='*80}

ìƒì„±: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

{rag}

{'='*80}
1ï¸âƒ£ ì‚¬ì—… ì •ë³´
{'='*80}

{info}

{'='*80}
2ï¸âƒ£ ì„¹í„° ì „ë¬¸ê°€ ë¶„ì„
{'='*80}

{analysis}

{'='*80}
3ï¸âƒ£ ì„¹í„° ì „ë¬¸ê°€ ê¶Œê³ ì•ˆ
{'='*80}

{recs}
"""
    
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:
        f.write(report)
        return f.name


def generate_html_report(rag, info, analysis, recs):
    def md_to_html(text):
        text = text.replace('ğŸ”´', '<span>ğŸ”´</span>')
        text = text.replace('ğŸŸ¡', '<span>ğŸŸ¡</span>')
        text = text.replace('ğŸŸ¢', '<span>ğŸŸ¢</span>')
        text = re.sub(r'^### (.*?)$', r'<h3>\1</h3>', text, flags=re.MULTILINE)
        text = re.sub(r'^## (.*?)$', r'<h2>\1</h2>', text, flags=re.MULTILINE)
        text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
        return f'<div>{text}</div>'
    
    html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>KOICA TAG v4.0 ì„¹í„° ì „ë¬¸ê°€</title>
    <style>
        body {{ font-family: 'Noto Sans KR', sans-serif; padding: 40px; max-width: 900px; margin: 0 auto; }}
        h1 {{ color: #2E7D32; }}
        h2 {{ color: #1976D2; margin-top: 40px; }}
        .section {{ background: #FAFAFA; padding: 25px; margin: 25px 0; border-radius: 10px; }}
    </style>
</head>
<body>
    <h1>ğŸ¯ KOICA TAG v4.0 ì„¹í„° ì „ë¬¸ê°€</h1>
    <p>ìƒì„±: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}</p>

    <div class="section">{md_to_html(rag)}</div>
    <h2>1ï¸âƒ£ ì‚¬ì—… ì •ë³´</h2>
    <div class="section">{md_to_html(info)}</div>
    <h2>2ï¸âƒ£ ì„¹í„° ì „ë¬¸ê°€ ë¶„ì„</h2>
    <div class="section">{md_to_html(analysis)}</div>
    <h2>3ï¸âƒ£ ì„¹í„° ì „ë¬¸ê°€ ê¶Œê³ ì•ˆ</h2>
    <div class="section">{md_to_html(recs)}</div>
</body>
</html>
"""
    
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.html', encoding='utf-8') as f:
        f.write(html_content)
        return f.name


demo = gr.Blocks(theme=gr.themes.Ocean(), title="KOICA TAG v4.0 ì„¹í„° ì „ë¬¸ê°€")

with demo:
    gr.Markdown("""
    # ğŸ¯ KOICA TAG v4.0 - ì„¹í„° ì „ë¬¸ê°€ ì§‘ì¤‘

    **ğŸ”¥ v4.0 ì£¼ìš” ë³€ê²½**:
    1. âœ… **PMC Agent ì œê±°**: LLM í˜¸ì¶œ 6íšŒ â†’ 1íšŒë¡œ ëŒ€í­ ì¶•ì†Œ
    2. âœ… **ì„¹í„° ì „ë¬¸ê°€ ì§‘ì¤‘**: ì„¹í„°ë³„ í•µì‹¬ ì´ìŠˆ + í•„ìˆ˜ ì§ˆë¬¸ ë¹¡ì„¸ê²Œ ê²€í† 
    3. âœ… **ì²˜ë¦¬ ì†ë„ í–¥ìƒ**: Agent ë¶€ë‹´ ê°ì†Œë¡œ ë¶„ì„ ì†ë„ ëŒ€í­ ê°œì„ 
    4. âœ… **ê²€í†  í’ˆì§ˆ ê°•í™”**: ì„¹í„° ì „ë¬¸ì„±ì— ì§‘ì¤‘í•œ ì‹¬ì¸µ ë¶„ì„

    **ê°œì„  íš¨ê³¼**:
    - âš¡ ì²˜ë¦¬ ì†ë„: Agent 6íšŒ â†’ 1íšŒ (ì•½ 5~6ë°° ë¹ ë¦„)
    - ğŸ¯ ì§‘ì¤‘ë„: PMC ì¼ë°˜ ê²€í†  ì œê±°, ì„¹í„° íŠ¹í™” ê²€í† ë§Œ ìˆ˜í–‰
    - ğŸ’¡ AI ë¶€ë‹´ ê°ì†Œ: í•œ ë²ˆì— í•˜ë‚˜ì˜ ì—­í• ë§Œ ìˆ˜í–‰ (ì •ì‹  ì°¨ë¦¼!)
    - ğŸ” ê²€í†  ê¹Šì´: ì„¹í„°ë³„ êµ­ì œ ê¸°ì¤€, ëª¨ë²” ì‚¬ë¡€ ì¤‘ì‹¬ ê²€í† 
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            pdf_input = gr.File(label="ğŸ“„ PDF", file_types=[".pdf"], type="filepath")
            status_box = gr.Textbox(label="ğŸ“Š ìƒíƒœ", interactive=False, lines=15)
    
    with gr.Tabs():
        with gr.Tab("0ï¸âƒ£ ì •ë³´"):
            rag_info = gr.Textbox(label="ë¶„ì„ ì •ë³´", lines=20, interactive=False)
        with gr.Tab("1ï¸âƒ£ í•µì‹¬"):
            info = gr.Textbox(label="ì‚¬ì—… ì •ë³´", lines=25, interactive=False)
        with gr.Tab("2ï¸âƒ£ ë¶„ì„"):
            analysis = gr.Textbox(label="ì„¹í„° ì „ë¬¸ê°€ ë¶„ì„ (í•µì‹¬ ì´ìŠˆ + í•„ìˆ˜ ì§ˆë¬¸)", lines=50, interactive=False)
        with gr.Tab("3ï¸âƒ£ ê¶Œê³ "):
            recs = gr.Textbox(label="ì„¹í„° ì „ë¬¸ê°€ ê¶Œê³ ì•ˆ", lines=45, interactive=False)
    
    with gr.Row():
        download_txt_btn = gr.DownloadButton(label="ğŸ“¥ TXT", visible=False)
        download_html_btn = gr.DownloadButton(label="ğŸŒ HTML", visible=False)
    
    def update_ui(pdf_file):
        outputs = None
        for outputs in upload_and_analyze_rag(pdf_file):
            yield outputs + (gr.DownloadButton(visible=False), gr.DownloadButton(visible=False))
        
        if outputs and outputs[2] and outputs[3] and outputs[4]:
            try:
                txt_path = generate_clean_report(outputs[1], outputs[2], outputs[3], outputs[4])
                html_path = generate_html_report(outputs[1], outputs[2], outputs[3], outputs[4])
                
                yield outputs + (
                    gr.DownloadButton(value=txt_path, visible=True),
                    gr.DownloadButton(value=html_path, visible=True)
                )
            except:
                yield outputs + (gr.DownloadButton(visible=False), gr.DownloadButton(visible=False))
    
    pdf_input.change(
        fn=update_ui,
        inputs=[pdf_input],
        outputs=[status_box, rag_info, info, analysis, recs, download_txt_btn, download_html_btn]
    )

print("=" * 80)
print("ğŸš€ KOICA TAG v4.0 (ì„¹í„° ì „ë¬¸ê°€ ì§‘ì¤‘)")
print("=" * 80)
print("\nğŸ”¥ v4.0 ì£¼ìš” ë³€ê²½:")
print("  - PMC Agent ì œê±° (LLM í˜¸ì¶œ 6íšŒ â†’ 1íšŒ)")
print("  - ì„¹í„° ì „ë¬¸ê°€ ë¶„ì„ë§Œ ì§‘ì¤‘ (ë¹¡ì„¼ ê²€í† )")
print("  - ì²˜ë¦¬ ì†ë„ ëŒ€í­ í–¥ìƒ (ì•½ 5~6ë°°)")
print("  - ì„¹í„°ë³„ í•µì‹¬ ì´ìŠˆ + í•„ìˆ˜ ì§ˆë¬¸ ê°•í™”")
print("  - AI ë¶€ë‹´ ê°ì†Œë¡œ ì •ì‹  ì°¨ë¦¼!")
print("\n" + "=" * 80)

demo.launch(share=True, debug=False, show_error=True)
