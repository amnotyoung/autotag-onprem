@echo off
REM ========================================
REM KOICA TAG v3.1 - Windows ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
REM ========================================

echo ğŸš€ KOICA TAG v3.1 ë¡œì»¬ í™˜ê²½ ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...
echo.

REM 1. Python ë²„ì „ í™•ì¸
echo 1ï¸âƒ£ Python ë²„ì „ í™•ì¸ ì¤‘...
python --version >nul 2>&1
if errorlevel 1 (
    echo âŒ Pythonì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!
    echo Python 3.8 ì´ìƒì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: https://www.python.org/downloads/
    pause
    exit /b 1
)

for /f "tokens=2" %%i in ('python --version') do set PYTHON_VERSION=%%i
echo âœ… Python %PYTHON_VERSION% ë°œê²¬
echo.

REM 2. NVIDIA GPU í™•ì¸
echo 2ï¸âƒ£ NVIDIA GPU í™•ì¸ ì¤‘...
nvidia-smi >nul 2>&1
if errorlevel 1 (
    echo âš ï¸  NVIDIA GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    echo GPU ì—†ì´ ì‹¤í–‰í•˜ë©´ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    set /p CONTINUE="ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/N): "
    if /i not "%CONTINUE%"=="Y" exit /b 1
    set HAS_GPU=false
    set TORCH_INDEX=
    set LLAMA_CPP_INDEX=
) else (
    for /f "tokens=*" %%i in ('nvidia-smi --query-gpu^=name --format^=csv,noheader') do set GPU_NAME=%%i
    for /f "tokens=*" %%i in ('nvidia-smi --query-gpu^=memory.total --format^=csv,noheader') do set GPU_MEMORY=%%i
    echo âœ… GPU ë°œê²¬: !GPU_NAME!
    echo    VRAM: !GPU_MEMORY!
    set HAS_GPU=true

    REM CUDA 12.1 ê¸°ë³¸ ì‚¬ìš© (WindowsëŠ” ëŒ€ë¶€ë¶„ ìµœì‹  CUDA ì‚¬ìš©)
    set TORCH_INDEX=https://download.pytorch.org/whl/cu121
    set LLAMA_CPP_INDEX=https://abetlen.github.io/llama-cpp-python/whl/cu121
)
echo.

REM 3. ê°€ìƒ í™˜ê²½ ìƒì„±
echo 4ï¸âƒ£ Python ê°€ìƒ í™˜ê²½ ìƒì„± ì¤‘...
if exist koica-env (
    echo âš ï¸  'koica-env' ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.
    set /p RECREATE="ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“œì‹œê² ìŠµë‹ˆê¹Œ? (Y/N): "
    if /i "%RECREATE%"=="Y" (
        rmdir /s /q koica-env
        python -m venv koica-env
        echo âœ… ê°€ìƒ í™˜ê²½ ì¬ìƒì„± ì™„ë£Œ
    ) else (
        echo ê¸°ì¡´ ê°€ìƒ í™˜ê²½ ì‚¬ìš©
    )
) else (
    python -m venv koica-env
    echo âœ… ê°€ìƒ í™˜ê²½ ìƒì„± ì™„ë£Œ
)
echo.

REM 4. ê°€ìƒ í™˜ê²½ í™œì„±í™”
echo 5ï¸âƒ£ ê°€ìƒ í™˜ê²½ í™œì„±í™” ì¤‘...
call koica-env\Scripts\activate.bat
if errorlevel 1 (
    echo âŒ ê°€ìƒ í™˜ê²½ í™œì„±í™” ì‹¤íŒ¨
    pause
    exit /b 1
)
echo âœ… ê°€ìƒ í™˜ê²½ í™œì„±í™” ì™„ë£Œ
echo.

REM 5. pip ì—…ê·¸ë ˆì´ë“œ
echo 6ï¸âƒ£ pip ì—…ê·¸ë ˆì´ë“œ ì¤‘...
python -m pip install --upgrade pip setuptools wheel -q
echo âœ… pip ì—…ê·¸ë ˆì´ë“œ ì™„ë£Œ
echo.

REM 6. PyTorch ì„¤ì¹˜
if "%HAS_GPU%"=="true" (
    echo 7ï¸âƒ£ PyTorch (CUDA ì§€ì›) ì„¤ì¹˜ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
    pip install torch torchvision torchaudio --index-url %TORCH_INDEX%
    echo âœ… PyTorch ì„¤ì¹˜ ì™„ë£Œ
) else (
    echo 7ï¸âƒ£ PyTorch (CPU ì „ìš©) ì„¤ì¹˜ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
    pip install torch torchvision torchaudio
    echo âœ… PyTorch ì„¤ì¹˜ ì™„ë£Œ
)
echo.

REM 7. PyTorch CUDA í…ŒìŠ¤íŠ¸
if "%HAS_GPU%"=="true" (
    echo 8ï¸âƒ£ PyTorch CUDA ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...
    python -c "import torch; assert torch.cuda.is_available(), 'CUDA ì‚¬ìš© ë¶ˆê°€'; print('âœ… PyTorch CUDA ì—°ê²° ì„±ê³µ')"
    if errorlevel 1 (
        echo âŒ PyTorchê°€ GPUë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
        echo LOCAL_SETUP.mdì˜ ë¬¸ì œ í•´ê²° ì„¹ì…˜ì„ ì°¸ê³ í•˜ì„¸ìš”.
        pause
        exit /b 1
    )
    echo âœ… PyTorch GPU ì—°ê²° í™•ì¸
) else (
    echo 8ï¸âƒ£ GPU ì—†ì´ ì§„í–‰
)
echo.

REM 8. llama-cpp-python ì„¤ì¹˜
if "%HAS_GPU%"=="true" (
    echo 9ï¸âƒ£ llama-cpp-python (CUDA ì§€ì›) ì„¤ì¹˜ ì¤‘...
    pip install llama-cpp-python --extra-index-url %LLAMA_CPP_INDEX%
    echo âœ… llama-cpp-python ì„¤ì¹˜ ì™„ë£Œ
) else (
    echo 9ï¸âƒ£ llama-cpp-python (CPU ì „ìš©) ì„¤ì¹˜ ì¤‘...
    pip install llama-cpp-python
    echo âœ… llama-cpp-python ì„¤ì¹˜ ì™„ë£Œ
)
echo.

REM 9. ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo ğŸ”Ÿ ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...
pip install pdfplumber gradio sentence-transformers huggingface-hub pandas numpy
echo âœ… ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ
echo.

REM 10. ì„¤ì¹˜ ê²€ì¦
echo 1ï¸âƒ£1ï¸âƒ£ ì„¤ì¹˜ ê²€ì¦ ì¤‘...
python -c "import torch; import pdfplumber; import gradio; from sentence_transformers import SentenceTransformer; from huggingface_hub import hf_hub_download; from llama_cpp import Llama; import pandas; import numpy; print('âœ… ëª¨ë“  íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì„±ê³µ'); print(f'âœ… GPU: {torch.cuda.get_device_name(0)}' if torch.cuda.is_available() else 'âš ï¸  GPU ê°€ì† ì—†ìŒ (CPU ëª¨ë“œ)'); print(f'âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB') if torch.cuda.is_available() else None"
echo.

REM ì™„ë£Œ ë©”ì‹œì§€
echo.
echo ========================================
echo ğŸ‰ ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
echo ========================================
echo.
echo ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•˜ì„¸ìš”:
echo.
echo   koica-env\Scripts\activate
echo   python autotag.py
echo.
echo ê°€ìƒ í™˜ê²½ì„ ì¢…ë£Œí•˜ë ¤ë©´:
echo   deactivate
echo.
echo ìì„¸í•œ ì‚¬ìš©ë²•ì€ LOCAL_SETUP.mdë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
echo.
pause
