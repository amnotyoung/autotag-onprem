# ğŸ–¥ï¸ ë¡œì»¬ í™˜ê²½ ì„¤ì • ê°€ì´ë“œ

KOICA TAG v3.1ì„ ë¡œì»¬ ì¥ì¹˜ì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ìƒì„¸ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ ì‚¬í•­ (Qwen2.5 32B ëª¨ë¸ ê¸°ì¤€)
- **GPU**: NVIDIA GPU (CUDA ì§€ì›)
  - **VRAM 16GB ì´ìƒ ê¶Œì¥** (RTX 4090, RTX 4080, RTX 3090 ë˜ëŠ” ë™ê¸‰)
  - **ìµœì†Œ VRAM 12GB** (RTX 3060 12GB - ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥)
  - âœ… **ë…¸íŠ¸ë¶ GPUì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥**
- **CUDA**: CUDA 11.8 ì´ìƒ
- **Python**: Python 3.8 - 3.11 (3.12ëŠ” ì¼ë¶€ íŒ¨í‚¤ì§€ í˜¸í™˜ì„± ì´ìŠˆ ê°€ëŠ¥)
- **RAM**: 16GB ì´ìƒ ê¶Œì¥
- **ì €ì¥ ê³µê°„**: 20GB ì´ìƒ (32B ëª¨ë¸ í¬ê¸°: ~15GB)

### ìš´ì˜ì²´ì œ
- Windows 10/11
- Linux (Ubuntu 20.04+)
- macOS (GPU ê°€ì† ì—†ìŒ, CPU ëª¨ë“œë¡œë§Œ ì‹¤í–‰ ê°€ëŠ¥ - ë§¤ìš° ëŠë¦¼)

## ğŸš€ ì„¤ì¹˜ ë°©ë²•

### 1ë‹¨ê³„: NVIDIA ë“œë¼ì´ë²„ ë° CUDA ì„¤ì¹˜

#### Windows
1. [NVIDIA ë“œë¼ì´ë²„ ë‹¤ìš´ë¡œë“œ](https://www.nvidia.com/Download/index.aspx)
2. [CUDA Toolkit ë‹¤ìš´ë¡œë“œ](https://developer.nvidia.com/cuda-downloads) (11.8 ì´ìƒ)
3. ì„¤ì¹˜ í›„ í™•ì¸:
```bash
nvidia-smi
nvcc --version
```

#### Linux (Ubuntu)
```bash
# NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜
sudo apt update
sudo apt install nvidia-driver-535

# CUDA ì„¤ì¹˜
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
sudo apt install cuda-toolkit-12-1

# í™•ì¸
nvidia-smi
nvcc --version
```

### 2ë‹¨ê³„: Python ê°€ìƒ í™˜ê²½ ìƒì„±

```bash
# Python 3.10 ì‚¬ìš© ê¶Œì¥
python3 --version

# ê°€ìƒ í™˜ê²½ ìƒì„±
python3 -m venv koica-env

# ê°€ìƒ í™˜ê²½ í™œì„±í™”
## Windows
koica-env\Scripts\activate

## Linux/macOS
source koica-env/bin/activate
```

### 3ë‹¨ê³„: PyTorch ì„¤ì¹˜ (CUDA ì§€ì›)

#### CUDA 11.8
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### CUDA 12.1
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

#### PyTorch ì„¤ì¹˜ í™•ì¸
```python
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')"
```

ì¶œë ¥ ì˜ˆì‹œ:
```
CUDA available: True
GPU: NVIDIA GeForce RTX 3080
```

### 4ë‹¨ê³„: llama-cpp-python ì„¤ì¹˜ (CUDA ì§€ì›)

#### CUDA 12.1
```bash
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
```

#### CUDA 11.8
```bash
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu118
```

#### ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ (ì„ íƒì‚¬í•­, ìµœì  ì„±ëŠ¥)
```bash
# Windows - Visual Studio Build Tools í•„ìš”
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --force-reinstall --no-cache-dir

# Linux
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### 5ë‹¨ê³„: ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

ë˜ëŠ” ê°œë³„ ì„¤ì¹˜:
```bash
pip install pdfplumber gradio sentence-transformers huggingface-hub pandas numpy
```

## ğŸ¯ ì‹¤í–‰ ë°©ë²•

### ê¸°ë³¸ ì‹¤í–‰

```bash
# ê°€ìƒ í™˜ê²½ í™œì„±í™” í™•ì¸
# Windows: koica-env\Scripts\activate
# Linux/macOS: source koica-env/bin/activate

# í”„ë¡œê·¸ë¨ ì‹¤í–‰
python autotag.py
```

### ì‹¤í–‰ í›„
1. í„°ë¯¸ë„ì— Gradio URLì´ í‘œì‹œë©ë‹ˆë‹¤:
   ```
   Running on local URL:  http://127.0.0.1:7860
   ```

2. ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ í•´ë‹¹ URL ì ‘ì†

3. PDF íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„ ì‹œì‘

## âš ï¸ ë¬¸ì œ í•´ê²°

### GPUë¥¼ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°

**ì¦ìƒ**: `AssertionError: âŒ GPU ëŸ°íƒ€ì„ì´ ì•„ë‹™ë‹ˆë‹¤!`

**í•´ê²° ë°©ë²•**:
1. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸: `nvidia-smi`
2. PyTorch CUDA ì„¤ì¹˜ í™•ì¸:
   ```python
   python -c "import torch; print(torch.cuda.is_available())"
   ```
3. CUDA ë²„ì „ê³¼ PyTorch ë²„ì „ ì¼ì¹˜ í™•ì¸

### VRAM ë¶€ì¡± ì˜¤ë¥˜

**ì¦ìƒ**: `CUDA out of memory`

**í•´ê²° ë°©ë²• (Qwen2.5 32B)**:
1. **ë” ì‘ì€ ì–‘ìí™” ì‚¬ìš©**:
   - Q3_K_M (í˜„ì¬, ~15GB) â†’ Q2_K (~12GB, í’ˆì§ˆ ì €í•˜)

2. `autotag.py`ì˜ `n_ctx` ê°’ ì¤„ì´ê¸°:
   ```python
   n_ctx=16384  # â†’ 8192 ë˜ëŠ” 4096ìœ¼ë¡œ ë³€ê²½
   ```

3. ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ (í¬ë¡¬, ê²Œì„, IDE ë“±)

4. **GPU ë©”ëª¨ë¦¬ í™•ì¸**:
   ```bash
   nvidia-smi
   ```

5. **ëŒ€ì•ˆ ëª¨ë¸**:
   - ë” ì‘ì€ ëª¨ë¸: Llama 3.1 8B (VRAM 8GB)
   - ë” í° ëª¨ë¸: Llama 3.1 70B (VRAM 40GB í•„ìš”, í´ë¼ìš°ë“œ ê¶Œì¥)

### llama-cpp-python ì„¤ì¹˜ ì˜¤ë¥˜

**Windows ì‚¬ìš©ì**:
1. [Visual Studio Build Tools](https://visualstudio.microsoft.com/downloads/) ì„¤ì¹˜
2. "Desktop development with C++" ì›Œí¬ë¡œë“œ ì„ íƒ
3. ì¬ë¶€íŒ… í›„ ë‹¤ì‹œ ì„¤ì¹˜

**Linux ì‚¬ìš©ì**:
```bash
sudo apt install build-essential cmake
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼

**í•´ê²° ë°©ë²•**:
1. Hugging Face ê³„ì • ìƒì„± ë° í† í° ë°œê¸‰
2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:
   ```bash
   export HF_TOKEN="your_token_here"
   ```

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### ëª¨ë¸ ê²½ë¡œ ë³€ê²½

`autotag.py`ì—ì„œ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜ ë³€ê²½:
```python
model_path = hf_hub_download(
    repo_id="Qwen/Qwen2.5-32B-Instruct-GGUF",
    filename="qwen2.5-32b-instruct-q3_k_m.gguf",
    local_dir="/your/custom/path/models"  # ì—¬ê¸°ë¥¼ ë³€ê²½
)
```

### ì„±ëŠ¥ íŠœë‹

`autotag.py`ì˜ LLM ì´ˆê¸°í™” ë¶€ë¶„ ìˆ˜ì •:
```python
llm = Llama(
    model_path=model_path,
    n_ctx=16384,        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (ë©”ëª¨ë¦¬ â†” ì„±ëŠ¥)
    n_gpu_layers=-1,    # -1 = ì „ì²´ GPU ì‚¬ìš©
    n_batch=512,        # ë°°ì¹˜ í¬ê¸° ì¦ê°€ ì‹œ ì†ë„ í–¥ìƒ
    n_threads=4,        # CPU ìŠ¤ë ˆë“œ ìˆ˜
    use_mlock=True,     # RAM ê³ ì • (ë¹ ë¦„)
    verbose=False
)
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### Qwen2.5 32B (Q3_K_M) - í˜„ì¬ ë²„ì „
| GPU | VRAM | ì§€ì› ì—¬ë¶€ | ì²˜ë¦¬ ì‹œê°„ (30í˜ì´ì§€ PDF) |
|-----|------|----------|------------------------|
| RTX 4090 Desktop | 24GB | âœ… | ~4-5ë¶„ |
| RTX 4090 Laptop | 16GB | âœ… | ~5-7ë¶„ |
| RTX 4080 Laptop | 12GB | âœ… | ~6-8ë¶„ |
| RTX 3090 | 24GB | âœ… | ~5-7ë¶„ |
| RTX 3080 | 10GB | âŒ | ë©”ëª¨ë¦¬ ë¶€ì¡± |
| RTX 3060 12GB | 12GB | âš ï¸ | ~8-12ë¶„ |

### ë‹¤ë¥¸ ëª¨ë¸ ë¹„êµ
| ëª¨ë¸ | VRAM ìš”êµ¬ | ì„±ëŠ¥ | ì‹¤í–‰ ê°€ëŠ¥ GPU |
|------|----------|------|-------------|
| Llama 3.1 8B | 8GB | â­â­â­ | RTX 3060+ |
| **Qwen2.5 32B** | **15GB** | **â­â­â­â­** | **RTX 4090 Laptop+** |
| Llama 3.1 70B | 40GB | â­â­â­â­â­ | A100, H100 |

âœ… **32B ëª¨ë¸ ê¶Œì¥**: ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©´ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥

## ğŸ“ ì§€ì›

- GitHub Issues: [í”„ë¡œì íŠ¸ ì´ìŠˆ](https://github.com/amnotyoung/autotag/issues)
- ë¬¸ì„œ: README.md

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì„¤ì¹˜ ì™„ë£Œ ì „ í™•ì¸:
- [ ] NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ë¨ (`nvidia-smi` ì‘ë™)
- [ ] CUDA ì„¤ì¹˜ë¨ (`nvcc --version` ì‘ë™)
- [ ] Python ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”
- [ ] PyTorch CUDA ë²„ì „ ì„¤ì¹˜ (`torch.cuda.is_available() == True`)
- [ ] llama-cpp-python CUDA ë²„ì „ ì„¤ì¹˜
- [ ] requirements.txt íŒ¨í‚¤ì§€ ì„¤ì¹˜
- [ ] `python autotag.py` ì‹¤í–‰ ì‹œ GPU ì¸ì‹ í™•ì¸

ëª¨ë“  ì²´í¬ë°•ìŠ¤ë¥¼ í™•ì¸í–ˆë‹¤ë©´ ì¤€ë¹„ ì™„ë£Œì…ë‹ˆë‹¤! ğŸ‰
