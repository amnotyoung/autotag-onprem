# ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

Qwen2.5 32B ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œì…ë‹ˆë‹¤.

## âš ï¸ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”!

### GPU í™•ì¸
```bash
nvidia-smi
```

**í•„ìš”í•œ GPU VRAM**:
- âœ… **16GB ì´ìƒ**: RTX 4090 Laptop, RTX 4080, RTX 3090 â†’ ì™„ë²½í•˜ê²Œ ì‹¤í–‰ ê°€ëŠ¥
- âš ï¸ **12GB**: RTX 3060 12GB, RTX 4060 Ti â†’ ì‹¤í–‰ ê°€ëŠ¥ (ì•½ê°„ ëŠë¦¼)
- âŒ **12GB ë¯¸ë§Œ**: RTX 3060 8GB, RTX 3070 â†’ **32B ëª¨ë¸ ì‹¤í–‰ ë¶ˆê°€** (8B ëª¨ë¸ ê¶Œì¥)

---

## ğŸ“¦ 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •

### Python ê°€ìƒ í™˜ê²½ ìƒì„±
```bash
# í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™
cd autotag-onprem

# ê°€ìƒ í™˜ê²½ ìƒì„±
python3 -m venv koica-env

# í™œì„±í™”
# Windows:
koica-env\Scripts\activate

# Linux/macOS:
source koica-env/bin/activate
```

### PyTorch ì„¤ì¹˜ (CUDA ì§€ì›)
```bash
# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### GPU ì¸ì‹ í™•ì¸
```bash
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0)}'); print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB')"
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
CUDA: True
GPU: NVIDIA A100-SXM4-40GB
VRAM: 40.0GB
```

---

## ğŸ”§ 2ë‹¨ê³„: íŒ¨í‚¤ì§€ ì„¤ì¹˜

### llama-cpp-python (CUDA ì§€ì›)
```bash
# CUDA 12.1
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121

# CUDA 11.8
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu118
```

### ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€
```bash
pip install pdfplumber gradio sentence-transformers huggingface-hub pandas numpy
```

---

## ğŸ¯ 3ë‹¨ê³„: ì‹¤í–‰

```bash
python autotag.py
```

### ì²« ì‹¤í–‰ ì‹œ ì¼ì–´ë‚˜ëŠ” ì¼
1. **ëª¨ë¸ ë‹¤ìš´ë¡œë“œ** (~15GB, 5-10ë¶„ ì†Œìš”)
   ```
   ğŸ“¥ Qwen2.5 32B ë‹¤ìš´ë¡œë“œ ì¤‘...
   Downloading...  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
   ```

2. **GPU ë©”ëª¨ë¦¬ í• ë‹¹**
   ```
   âœ… GPU: NVIDIA GeForce RTX 4090 Laptop
   âœ… VRAM: 16.0GB
   ğŸ”„ LLM ì´ˆê¸°í™” ì¤‘...
   âœ… LLM ì¤€ë¹„ ì™„ë£Œ!
   ```

3. **Gradio ì¸í„°í˜ì´ìŠ¤ ì‹œì‘**
   ```
   Running on local URL:  http://127.0.0.1:7860
   ```

### ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
1. ì›¹ ë¸Œë¼ìš°ì € ì—´ê¸°
2. `http://127.0.0.1:7860` ì ‘ì†
3. PDF íŒŒì¼ ì—…ë¡œë“œ
4. ë¶„ì„ ì‹œì‘ (30í˜ì´ì§€ ê¸°ì¤€ 5-10ë¶„)

---

## ğŸ› ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: "GPU ëŸ°íƒ€ì„ì´ ì•„ë‹™ë‹ˆë‹¤!"
```bash
# PyTorch CUDA ì„¤ì¹˜ í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"

# Falseê°€ ë‚˜ì˜¤ë©´ PyTorch ì¬ì„¤ì¹˜
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### ë¬¸ì œ 2: "CUDA out of memory"

**12GB GPU ì‚¬ìš©ì**:
`autotag.py`ì—ì„œ ë” ì‘ì€ ì–‘ìí™” ëª¨ë¸ë¡œ ë³€ê²½:
```python
# 40-42ë²ˆì§¸ ì¤„ ìˆ˜ì •
model_path = hf_hub_download(
    repo_id="bartowski/Qwen2.5-32B-Instruct-GGUF",
    filename="Qwen2.5-32B-Instruct-Q2_K.gguf",  # Q3_K_M â†’ Q2_K
    local_dir="./models"
)
```

**ê·¸ë˜ë„ ì•ˆ ë˜ë©´ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì¤„ì´ê¸°**:
```python
# 47-55ë²ˆì§¸ ì¤„ ìˆ˜ì •
llm = Llama(
    model_path=model_path,
    n_ctx=8192,  # 16384 â†’ 8192ë¡œ ë³€ê²½
    n_gpu_layers=-1,
    n_batch=512,
    n_threads=4,
    use_mlock=True,
    verbose=False
)
```

### ë¬¸ì œ 3: 12GB ë¯¸ë§Œ GPU
32B ëª¨ë¸ ëŒ€ì‹  8B ëª¨ë¸ ì‚¬ìš©:
```python
# autotag.py 40-42ë²ˆì§¸ ì¤„
model_path = hf_hub_download(
    repo_id="QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF",
    filename="Meta-Llama-3.1-8B-Instruct.Q6_K.gguf",
    local_dir="./models"
)
```

---

## ğŸ“Š ì‹¤í–‰ í™•ì¸

### ì •ìƒ ì‹¤í–‰ ì‹œ í„°ë¯¸ë„ ì¶œë ¥ ì˜ˆì‹œ:
```
âœ… GPU: NVIDIA A100-SXM4-40GB
âœ… VRAM: 40.0GB

âœ… GPU í™•ì¸ ì™„ë£Œ! íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ ì¤‘...

ğŸ“¥ Llama 3.1 70B ë‹¤ìš´ë¡œë“œ ì¤‘...
ğŸ”„ LLM ì´ˆê¸°í™” ì¤‘...
âœ… LLM ì¤€ë¹„ ì™„ë£Œ!

ğŸ”„ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”©...
âœ… í•œêµ­ì–´ ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ!

================================================================================
ğŸš€ KOICA TAG v3.1 (ì˜ˆì‹œ ë³µì‚¬ ë°©ì§€ ê°•í™”)
================================================================================

ğŸ”§ v3.1 ê°œì„ :
  - ì˜ˆì‹œë¥¼ í˜•ì‹ ê°€ì´ë“œë¡œ ë³€ê²½ (êµ¬ì²´ì  ë‚´ìš© ì œê±°)
  - ì˜ˆì‹œ ë‚´ìš© ë³µì‚¬ ì ˆëŒ€ ê¸ˆì§€ ëª…ì‹œ
  - ì˜ˆì‹œ ë³µì‚¬ ê²€ì¦ ë¡œì§ ì¶”ê°€
  - ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ë§Œ ì‚¬ìš© ê°•ì¡°

================================================================================
Running on local URL:  http://127.0.0.1:7860
Running on public URL: https://xxxxx.gradio.live
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹¤í–‰ ì „ í™•ì¸:
- [ ] GPU VRAM 16GB ì´ìƒ (ë˜ëŠ” 12GB + Q2_K ì–‘ìí™”)
- [ ] NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ë¨ (`nvidia-smi` ì‘ë™)
- [ ] Python ê°€ìƒ í™˜ê²½ í™œì„±í™”
- [ ] PyTorch CUDA ì„¤ì¹˜ (`torch.cuda.is_available() == True`)
- [ ] llama-cpp-python ì„¤ì¹˜
- [ ] ì €ì¥ ê³µê°„ 20GB ì´ìƒ í™•ë³´

ëª¨ë“  í•­ëª© í™•ì¸ ì™„ë£Œ â†’ `python autotag.py` ì‹¤í–‰!

---

## ğŸ’¡ íŒ

### ë” ë¹ ë¥¸ ë‹¤ìš´ë¡œë“œ
Hugging Face í† í° ì‚¬ìš©:
```bash
export HF_TOKEN="your_token_here"
python autotag.py
```

### ëª¨ë¸ ì €ì¥ ìœ„ì¹˜ í™•ì¸
```bash
ls -lh models/
```

### GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
í„°ë¯¸ë„ ìƒˆ ì°½ì—ì„œ:
```bash
watch -n 1 nvidia-smi
```

---

## ğŸ“ ë„ì›€ì´ í•„ìš”í•˜ë©´

1. `LOCAL_SETUP.md` - ìƒì„¸ ì„¤ì¹˜ ê°€ì´ë“œ
2. `README.md` - í”„ë¡œì íŠ¸ ì „ì²´ ë¬¸ì„œ
3. GitHub Issues - ë¬¸ì œ ë¦¬í¬íŠ¸

ğŸ‰ ì¤€ë¹„ë˜ì…¨ìœ¼ë©´ ì‹œì‘í•˜ì„¸ìš”!
