{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🎯 KOICA TAG v4.0 - 섹터 전문가 집중 + Qwen2.5 32B\n\n**🔥 v4.0 주요 변경**:\n1. ✅ **PMC Agent 제거**: LLM 호출 6회 → 1회로 대폭 축소\n2. ✅ **섹터 전문가 집중**: 섹터별 핵심 이슈 + 필수 질문 심층 검토\n3. ✅ **처리 속도 향상**: Agent 부담 감소로 약 5~6배 빠름\n4. ✅ **검토 품질 강화**: 섹터 전문성에 집중한 분석\n5. ✅ **Qwen2.5 32B**: 최신 모델, 우수한 성능 + 빠른 속도 (A100 최적)\n\n**해결된 문제**:\n- ❌ 복잡한 Multi-Agent → ✅ 단일 섹터 전문가 집중 분석\n- ❌ 느린 처리 속도 → ✅ 5~6배 빠른 분석\n- ❌ 작은 모델의 한계 → ✅ Qwen2.5 32B로 품질 향상"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ 패키지 설치\n",
    "print(\"📦 패키지 설치 중...\")\n",
    "!pip install -q pdfplumber gradio sentence-transformers huggingface-hub\n",
    "!pip install -q llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
    "!pip install -q pandas numpy\n",
    "print(\"✅ 설치 완료!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2️⃣ GitHub에서 최신 v4.0 코드 다운로드 및 실행 (Git Clone)\nprint(\"📥 v4.0 코드 다운로드 중 (Qwen2.5 32B)...\")\n\n# Git clone으로 최신 코드 받기 (캐시 우회)\n!rm -rf autotag-onprem autotag.py 2>/dev/null\n!git clone --depth 1 --branch claude/test-google-colab-011CUzGGNyvLiNjMy5Utmg6V https://github.com/amnotyoung/autotag-onprem.git\n!cp autotag-onprem/autotag.py .\n\n# 다운로드된 파일 확인\nprint(\"\\n✅ 다운로드 완료! 모델 확인 중...\")\n!head -50 autotag.py | grep -E \"(Qwen|LLaMA|Mistral)\" || echo \"모델 정보 확인 중...\"\n\nprint(\"\\n🚀 KOICA TAG v4.0 시작 (Qwen2.5 32B Instruct)...\\n\")\n%run autotag.py"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ℹ️ 사용 방법\n\n1. **위 2개 셀을 순서대로 실행** (Shift + Enter)\n2. **GPU 런타임 설정**: Runtime → Change runtime type → **GPU (A100 권장)**\n3. Gradio 인터페이스가 열리면 **PDF 업로드**\n4. 자동으로 TAG 분석 시작 (2-4분, v3.1 대비 5~6배 빠름)\n5. 분석 완료 후 **TXT/HTML 다운로드** 가능\n\n## 🔧 v4.0 개선사항\n\n- **섹터 전문가 집중**: PMC Agent 제거하고 섹터 전문가 분석에 집중\n- **처리 속도 향상**: LLM 호출 최소화로 약 **5~6배 빠름** (v3.1 대비)\n- **검토 품질 강화**: 섹터별 핵심 이슈 + 필수 질문 심층 검토\n- **Qwen2.5 32B**: 최신 모델로 우수한 성능 + 빠른 속도 (~18GB)\n\n## ⚠️ 중요사항\n\n- **GPU 런타임 필수**: Runtime → Change runtime type → **GPU** 선택\n- **20GB VRAM 권장**: A100 (40GB) 권장, T4 (16GB)도 가능\n- 첫 실행 시 Qwen2.5 32B 모델 다운로드로 **3-5분** 소요 (모델 크기: ~18GB)\n- 분석 시간: 30-50페이지 기준 약 **2-4분** (v3.1은 5-10분)\n- **Colab**: Pro/Pro+ 권장 (무료 T4도 가능)\n\n## 🐛 문제 해결\n\n| 문제 | 해결방법 |\n|------|----------|\n| GPU 에러 | Runtime → Change runtime type → GPU 확인 |\n| VRAM 부족 | GPU 레이어 조정 또는 A100으로 변경 |\n| 패키지 설치 에러 | 첫 번째 셀 다시 실행 |\n| 메모리 부족 | Runtime → Restart runtime 후 재시작 |\n| 타임아웃 | PDF 파일이 너무 크면 50페이지 이하로 분할 |\n| 다운로드 에러 | git clone 명령어 다시 실행 |\n\n## 📊 v3.1 vs v4.0 비교\n\n| 항목 | v3.1 | v4.0 |\n|------|------|------|\n| 분석 시간 | 5-10분 | **2-4분** |\n| LLM 호출 | 6회 | **1회** |\n| 모델 | Qwen2.5 32B (~15GB) | **Qwen2.5 32B (~18GB)** |\n| 권장 GPU | T4 (16GB) | **A100 (40GB), T4 가능** |\n| Agent 구조 | PMC + 섹터 | **섹터 전문가 집중** |\n| 검토 품질 | 일반 | **섹터 심층 분석** |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}