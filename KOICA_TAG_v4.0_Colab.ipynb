{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🎯 KOICA TAG v4.0 - 섹터 전문가 집중 + LLaMA 2 70B\n\n**🔥 v4.0 주요 변경**:\n1. ✅ **PMC Agent 제거**: LLM 호출 6회 → 1회로 대폭 축소\n2. ✅ **섹터 전문가 집중**: 섹터별 핵심 이슈 + 필수 질문 심층 검토\n3. ✅ **처리 속도 향상**: Agent 부담 감소로 약 5~6배 빠름\n4. ✅ **검토 품질 강화**: 섹터 전문성에 집중한 분석\n5. ✅ **LLaMA 2 70B Chat**: 대형 모델로 분석 품질 극대화 (40GB VRAM)\n\n**해결된 문제**:\n- ❌ 복잡한 Multi-Agent → ✅ 단일 섹터 전문가 집중 분석\n- ❌ 느린 처리 속도 → ✅ 5~6배 빠른 분석\n- ❌ 작은 모델의 한계 → ✅ LLaMA 2 70B로 품질 향상"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ 패키지 설치\n",
    "print(\"📦 패키지 설치 중...\")\n",
    "!pip install -q pdfplumber gradio sentence-transformers huggingface-hub\n",
    "!pip install -q llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
    "!pip install -q pandas numpy\n",
    "print(\"✅ 설치 완료!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2️⃣ GitHub에서 최신 v4.0 코드 다운로드 및 실행\nprint(\"📥 v4.0 코드 다운로드 중 (LLaMA 2 70B)...\")\n\n# 기존 파일 삭제 (캐시 방지)\n!rm -f autotag.py\n\n# 캐시 우회를 위해 타임스탬프 추가\nimport time\ntimestamp = int(time.time())\n!wget -q -O autotag.py \"https://raw.githubusercontent.com/amnotyoung/autotag-onprem/claude/test-google-colab-011CUzGGNyvLiNjMy5Utmg6V/autotag.py?t={timestamp}\"\n\n# 다운로드된 파일 확인\n!echo \"✅ 다운로드 완료! 모델 확인 중...\"\n!grep -A 2 \"📥 LLaMA 2 70B\" autotag.py || echo \"⚠️ 파일이 제대로 다운로드되지 않았을 수 있습니다.\"\n\nprint(\"\\n🚀 KOICA TAG v4.0 시작...\\n\")\n%run autotag.py"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ℹ️ 사용 방법\n\n1. **위 2개 셀을 순서대로 실행** (Shift + Enter)\n2. **GPU 런타임 설정**: Runtime → Change runtime type → **GPU (A100 또는 고용량 GPU 권장)**\n3. Gradio 인터페이스가 열리면 **PDF 업로드**\n4. 자동으로 TAG 분석 시작 (1-3분, v3.1 대비 5~6배 빠름)\n5. 분석 완료 후 **TXT/HTML 다운로드** 가능\n\n## 🔧 v4.0 개선사항\n\n- **섹터 전문가 집중**: PMC Agent 제거하고 섹터 전문가 분석에 집중\n- **처리 속도 향상**: LLM 호출 최소화로 약 **5~6배 빠름** (v3.1 대비)\n- **검토 품질 강화**: 섹터별 핵심 이슈 + 필수 질문 심층 검토\n- **LLaMA 2 70B Chat**: 대형 모델로 분석 품질 극대화 (~40GB)\n\n## ⚠️ 중요사항\n\n- **GPU 런타임 필수**: Runtime → Change runtime type → **GPU** 선택\n- **40GB VRAM 권장**: A100 (40GB) 권장, V100 (32GB)는 메모리 부족 가능\n- 첫 실행 시 LLaMA 2 70B 모델 다운로드로 **5-10분** 소요 (모델 크기: ~40GB)\n- 분석 시간: 30-50페이지 기준 약 **1-3분** (v3.1은 5-10분)\n- **Colab Pro/Pro+ 권장**: 무료 T4 (16GB)는 VRAM 부족으로 실행 불가\n\n## 🐛 문제 해결\n\n| 문제 | 해결방법 |\n|------|----------|\n| GPU 에러 | Runtime → Change runtime type → GPU 확인 |\n| VRAM 부족 | A100 또는 고용량 GPU로 변경 (Colab Pro/Pro+) |\n| 패키지 설치 에러 | 첫 번째 셀 다시 실행 |\n| 메모리 부족 | Runtime → Restart runtime 후 재시작 |\n| 타임아웃 | PDF 파일이 너무 크면 50페이지 이하로 분할 |\n| 다운로드 에러 | wget 명령어 다시 실행 |\n\n## 📊 v3.1 vs v4.0 비교\n\n| 항목 | v3.1 | v4.0 |\n|------|------|------|\n| 분석 시간 | 5-10분 | **1-3분** |\n| LLM 호출 | 6회 | **1회** |\n| 모델 | Qwen2.5 32B (~15GB) | **LLaMA 2 70B (~40GB)** |\n| 권장 GPU | T4 (16GB) | **A100 (40GB)** |\n| Agent 구조 | PMC + 섹터 | **섹터 전문가 집중** |\n| 검토 품질 | 일반 | **섹터 심층 분석 + 대형 모델** |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}